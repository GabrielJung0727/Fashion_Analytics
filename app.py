import streamlit as st
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix, accuracy_score
from scipy.sparse import hstack
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np
import time
import random
import plotly.graph_objects as go
from matplotlib import rcParams
import requests
import io
import zipfile
import os
import tensorflow as tf
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ - RandomForest, GradientBoosting, SVM, MLPë§Œ ì‚¬ìš©

# í°íŠ¸ ì„¤ì • - í•œê¸€ í‘œì‹œ ë¬¸ì œ í•´ê²°
plt.rcParams['font.family'] = 'AppleGothic, Arial'
plt.rcParams['axes.unicode_minus'] = False
# í•´ìƒë„ ì„¤ì •
plt.rcParams['figure.dpi'] = 200

# ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜
def generate_noise_image(size, complexity, color_scheme, df=None):
    # í¬ê¸°ì— ë”°ë¥¸ ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ ìƒì„± - í•´ìƒë„ ì¦ê°€
    image = np.zeros((size, size, 3), dtype=np.uint8)
    
    # ë³µì¡ë„ì— ë”°ë¥¸ ë…¸ì´ì¦ˆ ìƒì„±
    scale = 11 - complexity  # ë³µì¡ë„ ë°˜ì „ (ë†’ì„ìˆ˜ë¡ ë” ì„¸ë°€í•œ ë…¸ì´ì¦ˆ)
    
    for i in range(size):
        for j in range(size):
            if color_scheme == "ë¬´ì‘ìœ„":
                # ì™„ì „ ë¬´ì‘ìœ„ ìƒ‰ìƒ
                image[i, j, 0] = random.randint(0, 255)
                image[i, j, 1] = random.randint(0, 255)
                image[i, j, 2] = random.randint(0, 255)
                
            elif color_scheme == "ë°ì´í„° ê¸°ë°˜" and df is not None:
                # ë°ì´í„°ì…‹ì˜ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒ‰ìƒ ìƒì„±
                idx = (i * j) % len(df)
                price = df.iloc[idx]['Price (INR)']
                # ê°€ê²©ì— ë”°ë¥¸ ìƒ‰ìƒ (ë†’ì€ ê°€ê²© = ë” í‘¸ë¥¸ìƒ‰)
                image[i, j, 0] = min(255, int(price / 10))
                image[i, j, 1] = random.randint(0, 255)
                image[i, j, 2] = 255 - min(255, int(price / 10))
                
            elif color_scheme == "ë¸”ë£¨ìŠ¤ì¼€ì¼":
                # íŒŒë€ìƒ‰ ê³„ì—´ë¡œ ë³€í™”
                value = (i * j * complexity) % 255
                image[i, j, 0] = 0
                image[i, j, 1] = value // 2
                image[i, j, 2] = value
                
            elif color_scheme == "íˆíŠ¸ë§µ":
                # íˆíŠ¸ë§µ ìŠ¤íƒ€ì¼
                value = ((i + j) * complexity) % 255
                if value < 85:
                    image[i, j] = [value * 3, 0, 0]
                elif value < 170:
                    image[i, j] = [255, (value - 85) * 3, 0]
                else:
                    image[i, j] = [255, 255, (value - 170) * 3]
            
            # íŒ¨í„´ì— ë³€í™” ì¶”ê°€ (ë³µì¡ë„ì— ë”°ë¼)
            if (i + j) % scale == 0:
                image[i, j] = 255 - image[i, j]
    
    return image

# Fashion MNIST ë°ì´í„°ì…‹ ë¡œë“œ í•¨ìˆ˜
@st.cache_data
def load_fashion_mnist():
    # ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
    DATA_DIR = 'fashion_mnist_data'
    
    # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
    if not (os.path.exists(os.path.join(DATA_DIR, 'train_images.npy')) and 
            os.path.exists(os.path.join(DATA_DIR, 'train_labels.npy')) and
            os.path.exists(os.path.join(DATA_DIR, 'test_images.npy')) and
            os.path.exists(os.path.join(DATA_DIR, 'test_labels.npy'))):
        st.warning("Fashion MNIST ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
        with st.spinner("ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            import download_fashion_mnist
            download_fashion_mnist.download_and_process_data()
        st.success("ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    
    # ì €ì¥ëœ NumPy ë°°ì—´ ë¡œë“œ
    train_images = np.load(os.path.join(DATA_DIR, 'train_images.npy'))
    train_labels = np.load(os.path.join(DATA_DIR, 'train_labels.npy'))
    test_images = np.load(os.path.join(DATA_DIR, 'test_images.npy'))
    test_labels = np.load(os.path.join(DATA_DIR, 'test_labels.npy'))
    
    # í´ë˜ìŠ¤ ì´ë¦„ ì •ì˜
    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
    
    return (train_images, train_labels), (test_images, test_labels), class_names

# Fashion MNIST ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
def train_fashion_mnist_model(train_images, train_labels, test_images, test_labels, epochs=10):
    # ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    train_images_reshaped = train_images.reshape(train_images.shape[0], 28, 28, 1)
    test_images_reshaped = test_images.reshape(test_images.shape[0], 28, 28, 1)
    
    # CNN ëª¨ë¸ êµ¬ì¶•
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10)
    ])
    
    # ëª¨ë¸ ì»´íŒŒì¼
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    
    # í•™ìŠµ ìƒíƒœ í‘œì‹œë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆ
    progress_bar = st.progress(0)
    status_text = st.empty()
    metrics_container = st.empty()
    chart_container = st.empty()
    
    # í•™ìŠµ ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜
    epochs_range = range(epochs)
    train_loss_history = []
    train_acc_history = []
    val_loss_history = []
    val_acc_history = []
    
    # ì—í­ë³„ í•™ìŠµ í•¨ìˆ˜
    for epoch in range(epochs):
        status_text.text(f"ì—í­ {epoch+1}/{epochs} í•™ìŠµ ì¤‘...")
        
        # í•œ ì—í­ í•™ìŠµ
        history = model.fit(
            train_images_reshaped, train_labels,
            validation_data=(test_images_reshaped, test_labels),
            epochs=1,
            verbose=0
        )
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        train_loss_history.append(history.history['loss'][0])
        train_acc_history.append(history.history['accuracy'][0])
        val_loss_history.append(history.history['val_loss'][0])
        val_acc_history.append(history.history['val_accuracy'][0])
        
        # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
        progress_bar.progress((epoch + 1) / epochs)
        
        # ë©”íŠ¸ë¦­ í‘œì‹œ
        col1, col2 = st.columns(2)
        with col1:
            st.metric("í•™ìŠµ ì •í™•ë„", f"{train_acc_history[-1]:.4f}")
            st.metric("ê²€ì¦ ì •í™•ë„", f"{val_acc_history[-1]:.4f}")
        with col2:
            st.metric("í•™ìŠµ ì†ì‹¤", f"{train_loss_history[-1]:.4f}")
            st.metric("ê²€ì¦ ì†ì‹¤", f"{val_loss_history[-1]:.4f}")
        
        # í•™ìŠµ ì°¨íŠ¸ ì—…ë°ì´íŠ¸
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # ì •í™•ë„ ê·¸ë˜í”„
        ax1.plot(epochs_range[:epoch+1], train_acc_history, label='í•™ìŠµ ì •í™•ë„')
        ax1.plot(epochs_range[:epoch+1], val_acc_history, label='ê²€ì¦ ì •í™•ë„')
        ax1.set_xlabel('ì—í­')
        ax1.set_ylabel('ì •í™•ë„')
        ax1.set_ylim([0.5, 1])
        ax1.legend(loc='lower right')
        ax1.set_title('í•™ìŠµ ë° ê²€ì¦ ì •í™•ë„')
        
        # ì†ì‹¤ ê·¸ë˜í”„
        ax2.plot(epochs_range[:epoch+1], train_loss_history, label='í•™ìŠµ ì†ì‹¤')
        ax2.plot(epochs_range[:epoch+1], val_loss_history, label='ê²€ì¦ ì†ì‹¤')
        ax2.set_xlabel('ì—í­')
        ax2.set_ylabel('ì†ì‹¤')
        ax2.legend(loc='upper right')
        ax2.set_title('í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤')
        
        chart_container.pyplot(fig)
        plt.close(fig)
    
    status_text.text("í•™ìŠµ ì™„ë£Œ!")
    
    # ìµœì¢… í‰ê°€
    test_loss, test_acc = model.evaluate(test_images_reshaped, test_labels, verbose=0)
    
    return model, test_acc

# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
def visualize_predictions(model, test_images, test_labels, class_names, num_examples=16):
    # ì´ë¯¸ì§€ ë°ì´í„° í˜•íƒœ ë³€í™˜
    test_images_reshaped = test_images[:num_examples].reshape(num_examples, 28, 28, 1)
    
    # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
    probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])
    predictions = probability_model.predict(test_images_reshaped)
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    fig = plt.figure(figsize=(10, 10))
    for i in range(num_examples):
        plt.subplot(4, 4, i+1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(test_images[i], cmap=plt.cm.binary)
        
        predicted_label = np.argmax(predictions[i])
        true_label = test_labels[i]
        
        color = 'blue' if predicted_label == true_label else 'red'
        plt.xlabel(f"{class_names[predicted_label]} ({class_names[true_label]})", color=color)
    
    return fig

# í†µí•© ë°ì´í„°ì…‹ ë¡œë“œ í•¨ìˆ˜
@st.cache_data
def load_integrated_datasets():
    """ëª¨ë“  ë°ì´í„°ì…‹ì„ í†µí•©í•˜ì—¬ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    all_datasets = []
    dataset_info = []
    
    # 1. ê¸°ë³¸ Myntra ë°ì´í„°ì…‹
    try:
        df_myntra = pd.read_csv("myntra_products_catalog.csv")
        df_myntra.dropna(subset=['PrimaryColor'], inplace=True)
        df_myntra['DataSource'] = 'Myntra'
        df_myntra['PriceCategory'] = df_myntra['Price (INR)'].apply(lambda x: 'Low' if x <= 500 else ('Medium' if x <= 1500 else 'High'))
        
        # ê¸°ëŒ€ íŒë§¤ëŸ‰ ë“± ì¶”ê°€ íŠ¹ì„± ìƒì„±
        np.random.seed(42)
        df_myntra['ExpectedCustomers'] = np.random.randint(50, 5000, size=len(df_myntra))
        df_myntra['ConversionRate'] = np.random.uniform(0.01, 0.25, size=len(df_myntra))
        df_myntra['ExpectedSales'] = (df_myntra['ExpectedCustomers'] * df_myntra['ConversionRate']).astype(int)
        df_myntra['ExpectedRevenue'] = df_myntra['ExpectedSales'] * df_myntra['Price (INR)']
        
        all_datasets.append(df_myntra)
        dataset_info.append({
            'name': 'Myntra íŒ¨ì…˜ ë°ì´í„°',
            'samples': len(df_myntra),
            'brands': len(df_myntra['ProductBrand'].unique()),
            'avg_price': df_myntra['Price (INR)'].mean(),
            'source_file': 'myntra_products_catalog.csv'
        })
    except Exception as e:
        st.warning(f"Myntra ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 2. H&M ë°ì´í„°ì…‹ ì‹œë®¬ë ˆì´ì…˜
    try:
        np.random.seed(123)
        h_m_data = {
            'ProductName': [f'H&M Fashion Item {i}' for i in range(800)],
            'ProductBrand': np.random.choice(['H&M', 'H&M Premium', 'H&M Basic', 'H&M Trend'], size=800),
            'Gender': np.random.choice(['Men', 'Women', 'Unisex'], size=800),
            'PrimaryColor': np.random.choice(['Black', 'White', 'Red', 'Blue', 'Green', 'Gray', 'Pink'], size=800),
            'Price (INR)': np.random.uniform(200, 4000, size=800),
            'NumImages': np.random.randint(1, 8, size=800),
            'DataSource': 'H&M'
        }
        df_hm = pd.DataFrame(h_m_data)
        df_hm['PriceCategory'] = df_hm['Price (INR)'].apply(lambda x: 'Low' if x <= 500 else ('Medium' if x <= 1500 else 'High'))
        
        np.random.seed(124)
        df_hm['ExpectedCustomers'] = np.random.randint(30, 3000, size=len(df_hm))
        df_hm['ConversionRate'] = np.random.uniform(0.02, 0.20, size=len(df_hm))
        df_hm['ExpectedSales'] = (df_hm['ExpectedCustomers'] * df_hm['ConversionRate']).astype(int)
        df_hm['ExpectedRevenue'] = df_hm['ExpectedSales'] * df_hm['Price (INR)']
        
        all_datasets.append(df_hm)
        dataset_info.append({
            'name': 'H&M íŒ¨ì…˜ ë°ì´í„°',
            'samples': len(df_hm),
            'brands': len(df_hm['ProductBrand'].unique()),
            'avg_price': df_hm['Price (INR)'].mean(),
            'source_file': 'ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°'
        })
    except Exception as e:
        st.warning(f"H&M ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
    
    # 3. ASOS ë°ì´í„°ì…‹ ì‹œë®¬ë ˆì´ì…˜
    try:
        np.random.seed(456)
        asos_data = {
            'ProductName': [f'ASOS Style Product {i}' for i in range(600)],
            'ProductBrand': np.random.choice(['ASOS', 'ASOS Design', 'ASOS Premium', 'ASOS Curve'], size=600),
            'Gender': np.random.choice(['Male', 'Female'], size=600),
            'PrimaryColor': np.random.choice(['Black', 'White', 'Navy', 'Beige', 'Brown', 'Burgundy'], size=600),
            'Price (INR)': np.random.uniform(300, 6000, size=600),
            'NumImages': np.random.randint(2, 12, size=600),
            'DataSource': 'ASOS'
        }
        df_asos = pd.DataFrame(asos_data)
        df_asos['PriceCategory'] = df_asos['Price (INR)'].apply(lambda x: 'Low' if x <= 500 else ('Medium' if x <= 1500 else 'High'))
        
        np.random.seed(457)
        df_asos['ExpectedCustomers'] = np.random.randint(40, 4000, size=len(df_asos))
        df_asos['ConversionRate'] = np.random.uniform(0.015, 0.30, size=len(df_asos))
        df_asos['ExpectedSales'] = (df_asos['ExpectedCustomers'] * df_asos['ConversionRate']).astype(int)
        df_asos['ExpectedRevenue'] = df_asos['ExpectedSales'] * df_asos['Price (INR)']
        
        all_datasets.append(df_asos)
        dataset_info.append({
            'name': 'ASOS íŒ¨ì…˜ ë°ì´í„°',
            'samples': len(df_asos),
            'brands': len(df_asos['ProductBrand'].unique()),
            'avg_price': df_asos['Price (INR)'].mean(),
            'source_file': 'ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°'
        })
    except Exception as e:
        st.warning(f"ASOS ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
    
    # 4. Fashion Images ë°ì´í„°ì…‹ ì‹œë®¬ë ˆì´ì…˜
    try:
        np.random.seed(789)
        fashion_img_data = {
            'ProductName': [f'Fashion Image Product {i}' for i in range(500)],
            'ProductBrand': np.random.choice(['Brand A', 'Brand B', 'Brand C', 'Brand D', 'Brand E'], size=500),
            'Gender': np.random.choice(['Men', 'Women'], size=500),
            'PrimaryColor': np.random.choice(['Black', 'White', 'Red', 'Blue', 'Green', 'Yellow'], size=500),
            'Price (INR)': np.random.uniform(150, 5000, size=500),
            'NumImages': np.random.randint(3, 15, size=500),
            'DataSource': 'Fashion_Images'
        }
        df_fashion = pd.DataFrame(fashion_img_data)
        df_fashion['PriceCategory'] = df_fashion['Price (INR)'].apply(lambda x: 'Low' if x <= 500 else ('Medium' if x <= 1500 else 'High'))
        
        np.random.seed(790)
        df_fashion['ExpectedCustomers'] = np.random.randint(25, 2500, size=len(df_fashion))
        df_fashion['ConversionRate'] = np.random.uniform(0.01, 0.35, size=len(df_fashion))
        df_fashion['ExpectedSales'] = (df_fashion['ExpectedCustomers'] * df_fashion['ConversionRate']).astype(int)
        df_fashion['ExpectedRevenue'] = df_fashion['ExpectedSales'] * df_fashion['Price (INR)']
        
        all_datasets.append(df_fashion)
        dataset_info.append({
            'name': 'Fashion Images ë°ì´í„°',
            'samples': len(df_fashion),
            'brands': len(df_fashion['ProductBrand'].unique()),
            'avg_price': df_fashion['Price (INR)'].mean(),
            'source_file': 'ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°'
        })
    except Exception as e:
        st.warning(f"Fashion Images ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
    
    # ëª¨ë“  ë°ì´í„°ì…‹ í†µí•©
    if all_datasets:
        integrated_df = pd.concat(all_datasets, ignore_index=True)
        
        # ë¸Œëœë“œì™€ ì„±ë³„ ì •ë³´ í†µì¼
        integrated_df['Gender'] = integrated_df['Gender'].replace({'Male': 'Men', 'Female': 'Women'})
        
        return integrated_df, dataset_info
    else:
        # ê¸°ë³¸ ë°ì´í„°ì…‹ì´ë¼ë„ ë¡œë“œ
        return load_single_dataset("ê¸°ë³¸ Myntra ë°ì´í„°ì…‹"), []

@st.cache_data  
def load_single_dataset(dataset_name="ê¸°ë³¸ Myntra ë°ì´í„°ì…‹"):
    if dataset_name == "ê¸°ë³¸ Myntra ë°ì´í„°ì…‹":
        df = pd.read_csv("myntra_products_catalog.csv")
        df.dropna(subset=['PrimaryColor'], inplace=True)
        df['PriceCategory'] = df['Price (INR)'].apply(lambda x: 'Low' if x <= 500 else ('Medium' if x <= 1500 else 'High'))
        np.random.seed(42)
        df['ExpectedCustomers'] = np.random.randint(50, 5000, size=len(df))
        df['ConversionRate'] = np.random.uniform(0.01, 0.25, size=len(df))
        df['ExpectedSales'] = (df['ExpectedCustomers'] * df['ConversionRate']).astype(int)
        df['ExpectedRevenue'] = df['ExpectedSales'] * df['Price (INR)']
        return df
        
    elif dataset_name == "H&M íŒ¨ì…˜ ë°ì´í„°ì…‹":
        # H&M ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ìºê¸€ APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì§ì ‘ URLì—ì„œ ë‹¤ìš´ë¡œë“œ)
        try:
            # ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if os.path.exists("handm_fashion_products.csv"):
                df = pd.read_csv("handm_fashion_products.csv")
            else:
                # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ìºê¸€ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì•¼ í•¨)
                st.warning("H&M ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                # ìƒ˜í”Œ ë°ì´í„° í”„ë ˆì„ ìƒì„±
                data = {
                    'product_code': [f'HM{i:06d}' for i in range(1000)],
                    'product_name': [f'H&M Fashion Product {i}' for i in range(1000)],
                    'brand': np.random.choice(['H&M', 'H&M Premium', 'H&M Basic'], size=1000),
                    'gender': np.random.choice(['Men', 'Women', 'Unisex'], size=1000),
                    'color': np.random.choice(['Black', 'White', 'Red', 'Blue', 'Green'], size=1000),
                    'price': np.random.uniform(10, 300, size=1000)
                }
                df = pd.DataFrame(data)
            
            # ê°€ê²© ì¹´í…Œê³ ë¦¬ ì¶”ê°€
            df['PriceCategory'] = df['price'].apply(lambda x: 'Low' if x <= 50 else ('Medium' if x <= 150 else 'High'))
            
            # í•„ìš”í•œ ì—´ ì´ë¦„ ë³€í™˜ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„±ì„ ìœ„í•´)
            df = df.rename(columns={
                'product_name': 'ProductName',
                'brand': 'ProductBrand',
                'gender': 'Gender',
                'color': 'PrimaryColor',
                'price': 'Price (INR)'
            })
            
            # ì´ë¯¸ì§€ ìˆ˜ ëœë¤ ìƒì„±
            df['NumImages'] = np.random.randint(1, 10, size=len(df))
            
            # ê¸°ëŒ€ íŒë§¤ëŸ‰ ë“± ì¶”ê°€ íŠ¹ì„± ìƒì„±
            np.random.seed(42)
            df['ExpectedCustomers'] = np.random.randint(50, 5000, size=len(df))
            df['ConversionRate'] = np.random.uniform(0.01, 0.25, size=len(df))
            df['ExpectedSales'] = (df['ExpectedCustomers'] * df['ConversionRate']).astype(int)
            df['ExpectedRevenue'] = df['ExpectedSales'] * df['Price (INR)']
            
            return df
            
        except Exception as e:
            st.error(f"H&M ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë°ì´í„°ì…‹ ë°˜í™˜
            return load_single_dataset("ê¸°ë³¸ Myntra ë°ì´í„°ì…‹")
    
    elif dataset_name == "ASOS íŒ¨ì…˜ ë°ì´í„°ì…‹":
        try:
            # ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if os.path.exists("asos_fashion_dataset.csv"):
                df = pd.read_csv("asos_fashion_dataset.csv")
            else:
                # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ìºê¸€ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì•¼ í•¨)
                st.warning("ASOS ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                # ìƒ˜í”Œ ë°ì´í„° í”„ë ˆì„ ìƒì„±
                data = {
                    'product_id': [f'ASOS{i:06d}' for i in range(1000)],
                    'name': [f'ASOS Fashion Item {i}' for i in range(1000)],
                    'brand': np.random.choice(['ASOS', 'ASOS Design', 'ASOS Premium'], size=1000),
                    'gender': np.random.choice(['Male', 'Female'], size=1000),
                    'colour': np.random.choice(['Black', 'White', 'Red', 'Blue', 'Green'], size=1000),
                    'price': np.random.uniform(10, 300, size=1000)
                }
                df = pd.DataFrame(data)
            
            # ê°€ê²© ì¹´í…Œê³ ë¦¬ ì¶”ê°€
            df['PriceCategory'] = df['price'].apply(lambda x: 'Low' if x <= 50 else ('Medium' if x <= 150 else 'High'))
            
            # í•„ìš”í•œ ì—´ ì´ë¦„ ë³€í™˜ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„±ì„ ìœ„í•´)
            df = df.rename(columns={
                'name': 'ProductName',
                'brand': 'ProductBrand',
                'gender': 'Gender',
                'colour': 'PrimaryColor',
                'price': 'Price (INR)'
            })
            
            # ì´ë¯¸ì§€ ìˆ˜ ëœë¤ ìƒì„±
            df['NumImages'] = np.random.randint(1, 10, size=len(df))
            
            # ê¸°ëŒ€ íŒë§¤ëŸ‰ ë“± ì¶”ê°€ íŠ¹ì„± ìƒì„±
            np.random.seed(42)
            df['ExpectedCustomers'] = np.random.randint(50, 5000, size=len(df))
            df['ConversionRate'] = np.random.uniform(0.01, 0.25, size=len(df))
            df['ExpectedSales'] = (df['ExpectedCustomers'] * df['ConversionRate']).astype(int)
            df['ExpectedRevenue'] = df['ExpectedSales'] * df['Price (INR)']
            
            return df
            
        except Exception as e:
            st.error(f"ASOS ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë°ì´í„°ì…‹ ë°˜í™˜
            return load_single_dataset("ê¸°ë³¸ Myntra ë°ì´í„°ì…‹")
    
    elif dataset_name == "Fashion Images ë°ì´í„°ì…‹":
        try:
            # ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if os.path.exists("fashion_images_metadata.csv"):
                df = pd.read_csv("fashion_images_metadata.csv")
            else:
                # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ìºê¸€ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì•¼ í•¨)
                st.warning("Fashion Images ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                # ìƒ˜í”Œ ë°ì´í„° í”„ë ˆì„ ìƒì„±
                data = {
                    'id': list(range(1000)),
                    'product_name': [f'Fashion Product with Image {i}' for i in range(1000)],
                    'brand': np.random.choice(['Brand A', 'Brand B', 'Brand C', 'Brand D'], size=1000),
                    'gender': np.random.choice(['Men', 'Women'], size=1000),
                    'color': np.random.choice(['Black', 'White', 'Red', 'Blue', 'Green'], size=1000),
                    'price': np.random.uniform(10, 300, size=1000),
                    'image_path': [f'images/product_{i}.jpg' for i in range(1000)]
                }
                df = pd.DataFrame(data)
            
            # ê°€ê²© ì¹´í…Œê³ ë¦¬ ì¶”ê°€
            df['PriceCategory'] = df['price'].apply(lambda x: 'Low' if x <= 50 else ('Medium' if x <= 150 else 'High'))
            
            # í•„ìš”í•œ ì—´ ì´ë¦„ ë³€í™˜ (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„±ì„ ìœ„í•´)
            df = df.rename(columns={
                'product_name': 'ProductName',
                'brand': 'ProductBrand',
                'gender': 'Gender',
                'color': 'PrimaryColor',
                'price': 'Price (INR)'
            })
            
            # ì´ë¯¸ì§€ ìˆ˜ëŠ” ì´ë¯¸ ì´ë¯¸ì§€ ê²½ë¡œê°€ ìˆìœ¼ë¯€ë¡œ 1ë¡œ ì„¤ì •
            df['NumImages'] = 1
            
            # ê¸°ëŒ€ íŒë§¤ëŸ‰ ë“± ì¶”ê°€ íŠ¹ì„± ìƒì„±
            np.random.seed(42)
            df['ExpectedCustomers'] = np.random.randint(50, 5000, size=len(df))
            df['ConversionRate'] = np.random.uniform(0.01, 0.25, size=len(df))
            df['ExpectedSales'] = (df['ExpectedCustomers'] * df['ConversionRate']).astype(int)
            df['ExpectedRevenue'] = df['ExpectedSales'] * df['Price (INR)']
            
            return df
            
        except Exception as e:
            st.error(f"Fashion Images ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë°ì´í„°ì…‹ ë°˜í™˜
            return load_single_dataset("ê¸°ë³¸ Myntra ë°ì´í„°ì…‹")
    
    # ê¸°ë³¸ê°’
    return load_single_dataset("ê¸°ë³¸ Myntra ë°ì´í„°ì…‹")

@st.cache_data
def get_dataset_stats(df):
    """ë°ì´í„°ì…‹ì˜ ì£¼ìš” í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    stats = {
        "ìƒ˜í”Œ ìˆ˜": len(df),
        "ë¸Œëœë“œ ìˆ˜": len(df['ProductBrand'].unique()),
        "ì„±ë³„ ì¢…ë¥˜": len(df['Gender'].unique()),
        "ìƒ‰ìƒ ì¢…ë¥˜": len(df['PrimaryColor'].unique()),
        "í‰ê·  ê°€ê²©": df['Price (INR)'].mean(),
        "ìµœì†Œ ê°€ê²©": df['Price (INR)'].min(),
        "ìµœëŒ€ ê°€ê²©": df['Price (INR)'].max(),
        "ì €ê°€ ìƒí’ˆ ë¹„ìœ¨": len(df[df['PriceCategory'] == 'Low']) / len(df) * 100,
        "ì¤‘ê°€ ìƒí’ˆ ë¹„ìœ¨": len(df[df['PriceCategory'] == 'Medium']) / len(df) * 100,
        "ê³ ê°€ ìƒí’ˆ ë¹„ìœ¨": len(df[df['PriceCategory'] == 'High']) / len(df) * 100
    }
    return stats

def prepare_model(df, test_size=0.2, show_progress=False, model_type="RandomForest"):
    le_brand = LabelEncoder()
    le_gender = LabelEncoder()
    le_color = LabelEncoder()

    df['ProductBrand'] = le_brand.fit_transform(df['ProductBrand'])
    df['Gender'] = le_gender.fit_transform(df['Gender'])
    df['PrimaryColor'] = le_color.fit_transform(df['PrimaryColor'])

    tfidf = TfidfVectorizer(max_features=100)
    
    if show_progress:
        progress_bar = st.progress(0)
        progress_text = st.empty()
        resource_usage = st.empty()
        progress_text.text("í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
        
        # GPU/CPU ì‚¬ìš©ëŸ‰ ì‹œê°í™” ì˜ì—­
        gpu_usage_chart = st.empty()
        
    tfidf_matrix = tfidf.fit_transform(df['ProductName'])
    
    if show_progress:
        progress_bar.progress(25)
        progress_text.text("íŠ¹ì„± ê²°í•© ì¤‘...")
        
        # GPU/CPU ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ì¸¡ì •í•´ì•¼ í•¨)
        fig = simulate_resource_usage(25)
        gpu_usage_chart.plotly_chart(fig, use_container_width=True)

    X_meta = df[['ProductBrand', 'Gender', 'PrimaryColor', 'NumImages']]
    X = hstack([tfidf_matrix, X_meta])
    y = df['PriceCategory']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    
    if show_progress:
        progress_bar.progress(50)
        progress_text.text(f"{model_type} ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        # GPU/CPU ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ì¸¡ì •í•´ì•¼ í•¨)
        fig = simulate_resource_usage(50)
        gpu_usage_chart.plotly_chart(fig, use_container_width=True)

    # ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
    if model_type == "GradientBoosting":
        X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train
        X_test_dense = X_test.toarray() if hasattr(X_test, 'toarray') else X_test
        
        model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            random_state=42
        )
        model.fit(X_train_dense, y_train)
        
    elif model_type == "SVM":
        from sklearn.svm import SVC
        from sklearn.preprocessing import StandardScaler
        
        X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train
        X_test_dense = X_test.toarray() if hasattr(X_test, 'toarray') else X_test
        
        # ë°ì´í„° ì •ê·œí™”
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_dense)
        X_test_scaled = scaler.transform(X_test_dense)
        
        model = SVC(probability=True, random_state=42, kernel='rbf', C=1.0)
        model.fit(X_train_scaled, y_train)
        
    elif model_type == "MLP":
        from sklearn.neural_network import MLPClassifier
        from sklearn.preprocessing import StandardScaler
        
        X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train
        X_test_dense = X_test.toarray() if hasattr(X_test, 'toarray') else X_test
        
        # ë°ì´í„° ì •ê·œí™”
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_dense)
        X_test_scaled = scaler.transform(X_test_dense)
        
        model = MLPClassifier(
            hidden_layer_sizes=(128, 64, 32),
            max_iter=500,
            random_state=42,
            alpha=0.001
        )
        model.fit(X_train_scaled, y_train)
        
    else:  # RandomForest (ê¸°ë³¸ê°’)
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
    
    if show_progress:
        progress_bar.progress(75)
        progress_text.text("ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        # GPU/CPU ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì‹œë®¬ë ˆì´ì…˜
        fig = simulate_resource_usage(75)
        gpu_usage_chart.plotly_chart(fig, use_container_width=True)
        
        time.sleep(0.5)  # ì‹œê°ì  íš¨ê³¼ë¥¼ ìœ„í•œ ì§€ì—°
        progress_bar.progress(100)
        progress_text.text(f"{model_type} ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
        
        # ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
        fig = simulate_resource_usage(100)
        gpu_usage_chart.plotly_chart(fig, use_container_width=True)
        
        time.sleep(0.5)
        progress_text.empty()
        progress_bar.empty()

    return model, le_brand, le_gender, le_color, tfidf, X_test, y_test, X_meta, y, X_train, y_train

# GPU/CPU ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì‹œë®¬ë ˆì´ì…˜ í•¨ìˆ˜
def simulate_resource_usage(progress_percent):
    # ì‹œë®¬ë ˆì´ì…˜ëœ ë°ì´í„° ìƒì„±
    # ì‹¤ì œë¡œëŠ” psutilì´ë‚˜ gputil ë“±ìœ¼ë¡œ ì¸¡ì •í•´ì•¼ í•¨
    x = list(range(50))
    
    # ì§„í–‰ë„ì— ë”°ë¼ ë‹¤ë¥¸ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ íŒ¨í„´ ìƒì„±
    if progress_percent <= 25:
        # ì´ˆê¸° ë‹¨ê³„: ë³€ë™ì´ ì ê³  ì¤‘ê°„ ì •ë„ì˜ ì‚¬ìš©ëŸ‰
        gpu_mem = [random.uniform(30, 50) for _ in range(50)]
        gpu_util = [random.uniform(20, 40) for _ in range(50)]
    elif progress_percent <= 50:
        # í•™ìŠµ ì´ˆê¸°: ë†’ì€ ì‚¬ìš©ëŸ‰ìœ¼ë¡œ ìƒìŠ¹
        gpu_mem = [random.uniform(60, 85) for _ in range(50)]
        gpu_util = [random.uniform(40, 65) for _ in range(50)]
    elif progress_percent <= 75:
        # í•™ìŠµ ì¤‘ê°„: ë§¤ìš° ë†’ì€ ì‚¬ìš©ëŸ‰
        gpu_mem = [random.uniform(75, 95) for _ in range(50)]
        gpu_util = [random.uniform(50, 80) for _ in range(50)]
    else:
        # í•™ìŠµ í›„ë°˜/í‰ê°€: ë‹¤ì‹œ ë‚®ì•„ì§€ëŠ” ì‚¬ìš©ëŸ‰
        gpu_mem = [random.uniform(40, 70) for _ in range(50)]
        gpu_util = [random.uniform(30, 50) for _ in range(50)]
    
    # í‰ê·  ì‚¬ìš©ëŸ‰ ê³„ì‚°
    avg_gpu_mem = sum(gpu_mem) / len(gpu_mem)
    avg_gpu_util = sum(gpu_util) / len(gpu_util)
    
    # Plotly ê·¸ë˜í”„ ìƒì„± - í•´ìƒë„ ì¦ê°€
    fig = go.Figure()
    
    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ íŠ¸ë ˆì´ìŠ¤
    fig.add_trace(go.Scatter(
        x=x, y=gpu_mem,
        line=dict(color='red', width=2),
        name='GPU MEM'
    ))
    
    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ì—­
    fig.add_trace(go.Scatter(
        x=x, y=[30 for _ in range(50)],
        line=dict(color='rgba(0,0,0,0)'),
        fill='tonexty', 
        fillcolor='rgba(255,0,0,0.1)',
        name='MEM ì˜ì—­'
    ))
    
    # GPU í™œìš©ë„ íŠ¸ë ˆì´ìŠ¤
    fig.add_trace(go.Scatter(
        x=x, y=gpu_util,
        line=dict(color='yellow', width=2),
        name='GPU UTIL'
    ))
    
    # í‰ê· ì„ ê³¼ í…ìŠ¤íŠ¸
    fig.add_hline(y=avg_gpu_mem, line_dash="dash", line_color="red", 
                annotation_text=f"AVG GPU MEM: {avg_gpu_mem:.1f}%", 
                annotation_position="top right")
    
    fig.add_hline(y=avg_gpu_util, line_dash="dash", line_color="yellow", 
                annotation_text=f"AVG GPU UTIL: {avg_gpu_util:.1f}%", 
                annotation_position="bottom right")
    
    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        title='AI í•™ìŠµ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§',
        xaxis_title='ì‹œê°„',
        yaxis_title='ì‚¬ìš©ë¥  (%)',
        height=300,  # ë†’ì´ ì¦ê°€
        margin=dict(l=0, r=0, t=30, b=0),
        plot_bgcolor='rgba(0,0,0,0.9)',
        paper_bgcolor='rgba(0,0,0,0.0)',
        font=dict(color='white', size=14),  # ê¸€ê¼´ í¬ê¸° ì¦ê°€
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        yaxis=dict(range=[0, 100])
    )
    
    # í•´ìƒë„ ì„¤ì •
    fig.update_layout(
        width=900,
        height=400
    )
    
    return fig

def classify_product_proba(model, le_brand, le_gender, le_color, tfidf, name, brand, gender, color, num_images):
    brand_enc = le_brand.transform([brand])[0]
    gender_enc = le_gender.transform([gender])[0]
    color_enc = le_color.transform([color])[0]

    tfidf_input = tfidf.transform([name])
    meta_input = [[brand_enc, gender_enc, color_enc, num_images]]
    X_input = hstack([tfidf_input, meta_input])
    prediction = model.predict(X_input)[0]
    proba = model.predict_proba(X_input)[0]
    return prediction, proba

# í†µí•© AI í•™ìŠµ ì‹œìŠ¤í…œ í´ë˜ìŠ¤
class IntegratedAILearningSystem:
    def __init__(self):
        self.models = {}
        self.ensemble_model = None
        self.text_model = None
        self.image_model = None
        self.meta_model = None
        self.scaler = StandardScaler()
        self.learning_history = []
        self.prediction_confidence = []
        
    def setup_structured_learning(self, X_structured, y):
        """ì •í˜• ë°ì´í„° í•™ìŠµ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)"""
        print("ğŸ”§ ì •í˜• í•™ìŠµ ì‹œìŠ¤í…œ ì„¤ì • ì¤‘...")
        
        # ì—¬ëŸ¬ ì •í˜• í•™ìŠµ ëª¨ë¸ë“¤
        self.models['random_forest'] = RandomForestClassifier(n_estimators=200, random_state=42)
        self.models['gradient_boost'] = GradientBoostingClassifier(n_estimators=100, random_state=42)
        self.models['svm'] = SVC(probability=True, random_state=42)
        self.models['mlp'] = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)
        self.models['ada_boost'] = AdaBoostClassifier(n_estimators=100, random_state=42)
        
        # ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
        
        # ë°ì´í„° ì •ê·œí™”
        X_scaled = self.scaler.fit_transform(X_structured)
        
        # ê° ëª¨ë¸ í•™ìŠµ
        structured_scores = {}
        for name, model in self.models.items():
            try:
                if name in ['svm', 'mlp']:
                    # SVMê³¼ MLPëŠ” ì •ê·œí™”ëœ ë°ì´í„° ì‚¬ìš©
                    model.fit(X_scaled, y)
                    score = model.score(X_scaled, y)
                else:
                    # ë‚˜ë¨¸ì§€ ëª¨ë¸ë“¤ì€ ì •ê·œí™”ë˜ì§€ ì•Šì€ ë°ì´í„° ì‚¬ìš©
                    model.fit(X_structured, y)
                    score = model.score(X_structured, y)
                structured_scores[name] = score
                print(f"   âœ… {name}: {score:.4f}")
            except Exception as e:
                print(f"   âŒ {name} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
        
        return structured_scores
    
    def setup_text_learning(self, text_data, y):
        """ë¹„ì •í˜• ë°ì´í„° í•™ìŠµ (í…ìŠ¤íŠ¸ ê¸°ë°˜)"""
        print("ğŸ“ í…ìŠ¤íŠ¸ í•™ìŠµ ì‹œìŠ¤í…œ ì„¤ì • ì¤‘...")
        
        # TF-IDF ë²¡í„°í™” (ê³ ê¸‰ ì„¤ì •)
        self.tfidf = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.95,
            stop_words='english'
        )
        
        X_text = self.tfidf.fit_transform(text_data)
        
        # í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ë“¤
        text_models = {
            'text_rf': RandomForestClassifier(n_estimators=150, random_state=42),
            'text_svm': SVC(probability=True, kernel='linear', random_state=42),
            'text_nb': LogisticRegression(max_iter=1000, random_state=42)
        }
        
        text_scores = {}
        for name, model in text_models.items():
            model.fit(X_text, y)
            score = model.score(X_text, y)
            text_scores[name] = score
            self.models[name] = model
            print(f"   âœ… {name}: {score:.4f}")
        
        return text_scores
    
    def setup_transfer_learning(self, X_combined, y):
        """ì „ì´í•™ìŠµ ì‹œìŠ¤í…œ"""
        print("ğŸ”„ ì „ì´í•™ìŠµ ì‹œìŠ¤í…œ ì„¤ì • ì¤‘...")
        
        # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” pre-trained ëª¨ë¸ ì‚¬ìš©)
        base_model = RandomForestClassifier(n_estimators=100, random_state=42)
        base_model.fit(X_combined, y)
        
        # Fine-tuningì„ ìœ„í•œ ì¶”ê°€ ë ˆì´ì–´
        fine_tuned_models = {
            'transfer_rf': RandomForestClassifier(
                n_estimators=300, 
                max_depth=15,
                min_samples_split=5,
                random_state=42
            ),
            'transfer_gb': GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                random_state=42
            )
        }
        
        transfer_scores = {}
        for name, model in fine_tuned_models.items():
            model.fit(X_combined, y)
            score = model.score(X_combined, y)
            transfer_scores[name] = score
            self.models[name] = model
            print(f"   âœ… {name}: {score:.4f}")
        
        return transfer_scores
    
    def create_ensemble_system(self, X_train, y_train):
        """ì•™ìƒë¸” í•™ìŠµ ì‹œìŠ¤í…œ ìƒì„±"""
        print("ğŸ¯ ì•™ìƒë¸” ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘...")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤ ì„ íƒ
        best_models = []
        for name, model in self.models.items():
            if hasattr(model, 'predict_proba'):
                best_models.append((name, model))
        
        # ë³´íŒ… ë¶„ë¥˜ê¸° ìƒì„±
        self.ensemble_model = VotingClassifier(
            estimators=best_models[:5],  # ìƒìœ„ 5ê°œ ëª¨ë¸ ì‚¬ìš©
            voting='soft'  # í™•ë¥  ê¸°ë°˜ íˆ¬í‘œ
        )
        
        self.ensemble_model.fit(X_train, y_train)
        ensemble_score = self.ensemble_model.score(X_train, y_train)
        print(f"   ğŸ† ì•™ìƒë¸” ëª¨ë¸ ì •í™•ë„: {ensemble_score:.4f}")
        
        return ensemble_score
    
    def adaptive_learning_rate(self, current_accuracy, target_accuracy=0.95):
        """ì ì‘í˜• í•™ìŠµë¥  ì¡°ì •"""
        if current_accuracy < target_accuracy:
            learning_rate = min(0.1, (target_accuracy - current_accuracy) * 2)
            return learning_rate
        return 0.01
    
    def predict_with_confidence(self, X_test):
        """ì‹ ë¢°ë„ì™€ í•¨ê»˜ ì˜ˆì¸¡"""
        if self.ensemble_model is None:
            return None, None
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        predictions = self.ensemble_model.predict(X_test)
        probabilities = self.ensemble_model.predict_proba(X_test)
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ìµœëŒ€ í™•ë¥ ê°’)
        confidence_scores = np.max(probabilities, axis=1)
        
        return predictions, confidence_scores
    
    def get_learning_insights(self):
        """í•™ìŠµ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        insights = {
            'total_models': len(self.models),
            'best_individual_model': max(self.models.items(), 
                                       key=lambda x: x[1].score(
                                           self.scaler.transform([[1,1,1,1]]) if hasattr(x[1], 'predict') else [[1]], 
                                           [1]
                                       ) if hasattr(x[1], 'score') else 0),
            'ensemble_improvement': 0.05,  # ì˜ˆì‹œê°’
            'learning_stability': np.std([0.85, 0.87, 0.89, 0.91, 0.93])  # ì˜ˆì‹œê°’
        }
        return insights

# í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì ìš© í•¨ìˆ˜
def apply_integrated_learning(df, test_size=0.2, show_progress=True):
    """í†µí•© AI í•™ìŠµ ì‹œìŠ¤í…œ ì ìš©"""
    
    if show_progress:
        st.subheader("ğŸš€ í†µí•© AI í•™ìŠµ ì‹œìŠ¤í…œ")
        progress_container = st.container()
        progress_bar = st.progress(0)
        status_text = st.empty()
        metrics_container = st.empty()
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    ai_system = IntegratedAILearningSystem()
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    df_processed = df.copy()
    le_brand = LabelEncoder()
    le_gender = LabelEncoder()
    le_color = LabelEncoder()
    
    df_processed['ProductBrand'] = le_brand.fit_transform(df_processed['ProductBrand'])
    df_processed['Gender'] = le_gender.fit_transform(df_processed['Gender'])
    df_processed['PrimaryColor'] = le_color.fit_transform(df_processed['PrimaryColor'])
    
    # 1. ì •í˜• ë°ì´í„° ì¤€ë¹„ (ë©”íƒ€ë°ì´í„°)
    X_structured = df_processed[['ProductBrand', 'Gender', 'PrimaryColor', 'NumImages']].values
    y = df_processed['PriceCategory']
    
    if show_progress:
        progress_bar.progress(20)
        status_text.text("1ë‹¨ê³„: ì •í˜• ë°ì´í„° í•™ìŠµ ì¤‘...")
    
    # ì •í˜• í•™ìŠµ
    structured_scores = ai_system.setup_structured_learning(X_structured, y)
    
    # 2. ë¹„ì •í˜• ë°ì´í„° í•™ìŠµ (í…ìŠ¤íŠ¸)
    if show_progress:
        progress_bar.progress(40)
        status_text.text("2ë‹¨ê³„: í…ìŠ¤íŠ¸ ë°ì´í„° í•™ìŠµ ì¤‘...")
    
    text_scores = ai_system.setup_text_learning(df_processed['ProductName'], y)
    
    # 3. íŠ¹ì„± ê²°í•©
    tfidf_matrix = ai_system.tfidf.transform(df_processed['ProductName'])
    X_combined = hstack([tfidf_matrix, X_structured])
    
    if show_progress:
        progress_bar.progress(60)
        status_text.text("3ë‹¨ê³„: ì „ì´í•™ìŠµ ì ìš© ì¤‘...")
    
    # ì „ì´í•™ìŠµ
    transfer_scores = ai_system.setup_transfer_learning(X_combined, y)
    
    # 4. ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X_combined, y, test_size=test_size, random_state=42
    )
    
    if show_progress:
        progress_bar.progress(80)
        status_text.text("4ë‹¨ê³„: ì•™ìƒë¸” ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘...")
    
    # ì•™ìƒë¸” ì‹œìŠ¤í…œ êµ¬ì¶•
    ensemble_score = ai_system.create_ensemble_system(X_train, y_train)
    
    # 5. ìµœì¢… í‰ê°€
    if show_progress:
        progress_bar.progress(100)
        status_text.text("5ë‹¨ê³„: ìµœì¢… í‰ê°€ ì™„ë£Œ!")
    
    # ì˜ˆì¸¡ ë° ì‹ ë¢°ë„ ê³„ì‚°
    predictions, confidence_scores = ai_system.predict_with_confidence(X_test)
    final_accuracy = accuracy_score(y_test, predictions)
    
    # ê²°ê³¼ ë°˜í™˜
    results = {
        'ai_system': ai_system,
        'structured_scores': structured_scores,
        'text_scores': text_scores,
        'transfer_scores': transfer_scores,
        'ensemble_score': ensemble_score,
        'final_accuracy': final_accuracy,
        'confidence_scores': confidence_scores,
        'X_test': X_test,
        'y_test': y_test,
        'predictions': predictions,
        'le_brand': le_brand,
        'le_gender': le_gender,
        'le_color': le_color
    }
    
    return results

# í•™ìŠµ ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜
def visualize_integrated_learning_results(results):
    """í†µí•© í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"""
    
    # 1. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
    st.subheader("ğŸ“Š í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„")
    
    # ëª¨ë“  ìŠ¤ì½”ì–´ í•©ì¹˜ê¸°
    all_scores = {}
    all_scores.update(results['structured_scores'])
    all_scores.update(results['text_scores'])
    all_scores.update(results['transfer_scores'])
    all_scores['ensemble'] = results['ensemble_score']
    all_scores['final_accuracy'] = results['final_accuracy']
    
    # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
    fig, ax = plt.subplots(figsize=(15, 8))
    models = list(all_scores.keys())
    scores = list(all_scores.values())
    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))
    
    bars = ax.bar(models, scores, color=colors)
    ax.set_xlabel('ëª¨ë¸/ì‹œìŠ¤í…œ')
    ax.set_ylabel('ì •í™•ë„')
    ax.set_title('í†µí•© AI í•™ìŠµ ì‹œìŠ¤í…œ - ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ')
    ax.set_ylim(0, 1)
    
    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    for bar, score in zip(bars, scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01, 
                f'{score:.3f}', ha='center', va='bottom')
    
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    st.pyplot(fig)
    
    # 2. í•™ìŠµ íƒ€ì…ë³„ ì„±ëŠ¥ ê·¸ë£¹í™”
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### ğŸ”§ ì •í˜• í•™ìŠµ")
        structured_avg = np.mean(list(results['structured_scores'].values()))
        st.metric("í‰ê·  ì •í™•ë„", f"{structured_avg:.3f}")
        for name, score in results['structured_scores'].items():
            st.write(f"- {name}: {score:.3f}")
    
    with col2:
        st.markdown("### ğŸ“ í…ìŠ¤íŠ¸ í•™ìŠµ")
        text_avg = np.mean(list(results['text_scores'].values()))
        st.metric("í‰ê·  ì •í™•ë„", f"{text_avg:.3f}")
        for name, score in results['text_scores'].items():
            st.write(f"- {name}: {score:.3f}")
    
    with col3:
        st.markdown("### ğŸ”„ ì „ì´ í•™ìŠµ")
        transfer_avg = np.mean(list(results['transfer_scores'].values()))
        st.metric("í‰ê·  ì •í™•ë„", f"{transfer_avg:.3f}")
        for name, score in results['transfer_scores'].items():
            st.write(f"- {name}: {score:.3f}")
    
    # 3. ì‹ ë¢°ë„ ë¶„í¬
    st.subheader("ğŸ¯ ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„ì„")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ì‹ ë¢°ë„ íˆìŠ¤í† ê·¸ë¨
    ax1.hist(results['confidence_scores'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.set_xlabel('ì‹ ë¢°ë„ ì ìˆ˜')
    ax1.set_ylabel('ë¹ˆë„')
    ax1.set_title('ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„í¬')
    ax1.axvline(np.mean(results['confidence_scores']), color='red', linestyle='--', 
               label=f'í‰ê· : {np.mean(results["confidence_scores"]):.3f}')
    ax1.legend()
    
    # ì‹ ë¢°ë„ vs ì •í™•ë„
    correct_predictions = (results['predictions'] == results['y_test']).astype(int)
    ax2.scatter(results['confidence_scores'], correct_predictions, alpha=0.6)
    ax2.set_xlabel('ì‹ ë¢°ë„ ì ìˆ˜')
    ax2.set_ylabel('ì˜ˆì¸¡ ì •í™•ì„± (1=ë§ìŒ, 0=í‹€ë¦¼)')
    ax2.set_title('ì‹ ë¢°ë„ vs ì˜ˆì¸¡ ì •í™•ì„±')
    
    plt.tight_layout()
    st.pyplot(fig)
    
    # 4. ì•™ìƒë¸” íš¨ê³¼ ë¶„ì„
    st.subheader("ğŸ† ì•™ìƒë¸” í•™ìŠµ íš¨ê³¼")
    
    individual_max = max(max(results['structured_scores'].values()),
                        max(results['text_scores'].values()),
                        max(results['transfer_scores'].values()))
    
    improvement = results['final_accuracy'] - individual_max
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ìµœê³  ê°œë³„ ëª¨ë¸", f"{individual_max:.3f}")
    with col2:
        st.metric("ì•™ìƒë¸” ëª¨ë¸", f"{results['final_accuracy']:.3f}")
    with col3:
        st.metric("ì„±ëŠ¥ í–¥ìƒ", f"{improvement:.3f}", delta=f"{improvement:.3f}")
    
    # 5. í•™ìŠµ ì‹œìŠ¤í…œ ìš”ì•½
    st.subheader("ğŸ“‹ í†µí•© í•™ìŠµ ì‹œìŠ¤í…œ ìš”ì•½")
    
    summary_data = {
        'í•™ìŠµ ë°©ë²•': ['ì •í˜• í•™ìŠµ', 'í…ìŠ¤íŠ¸ í•™ìŠµ', 'ì „ì´ í•™ìŠµ', 'ì•™ìƒë¸” í•™ìŠµ'],
        'ëª¨ë¸ ìˆ˜': [len(results['structured_scores']), 
                  len(results['text_scores']), 
                  len(results['transfer_scores']), 1],
        'ìµœê³  ì„±ëŠ¥': [max(results['structured_scores'].values()),
                   max(results['text_scores'].values()),
                   max(results['transfer_scores'].values()),
                   results['final_accuracy']],
        'íŠ¹ì§•': ['ë©”íƒ€ë°ì´í„° ê¸°ë°˜', 'TF-IDF + N-gram', 'Fine-tuning', 'Soft Voting']
    }
    
    summary_df = pd.DataFrame(summary_data)
    st.table(summary_df)
    
    return results

st.set_page_config(page_title="íŒ¨ì…˜ ê°€ê²©ëŒ€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ‘• íŒ¨ì…˜ ìƒí’ˆ ê°€ê²©ëŒ€ ì˜ˆì¸¡ ë° ì „ëµ ë¶„ì„")

with st.sidebar:
    st.header("ğŸš€ í†µí•© AI í•™ìŠµ ì‹œìŠ¤í…œ")
    
    # ë°ì´í„°ì…‹ ëª¨ë“œ ì„ íƒ
    data_mode = st.radio(
        "ë°ì´í„°ì…‹ ëª¨ë“œ",
        ["í†µí•© ë°ì´í„°ì…‹ (ê¶Œì¥)", "ê°œë³„ ë°ì´í„°ì…‹", "Fashion MNIST"]
    )
    
    if data_mode == "ê°œë³„ ë°ì´í„°ì…‹":
        # ê¸°ì¡´ ê°œë³„ ë°ì´í„°ì…‹ ì„ íƒ
        dataset_name = st.selectbox(
            "ë°ì´í„°ì…‹ ì„ íƒ", 
            ["ê¸°ë³¸ Myntra ë°ì´í„°ì…‹", "H&M íŒ¨ì…˜ ë°ì´í„°ì…‹", "ASOS íŒ¨ì…˜ ë°ì´í„°ì…‹", "Fashion Images ë°ì´í„°ì…‹"]
        )
    else:
        dataset_name = "í†µí•© ë°ì´í„°ì…‹"
    
    # ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
    available_models = ["RandomForest", "GradientBoosting", "SVM", "MLP"]
    
    model_type = st.selectbox("ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ", available_models)
    
    # ì§€ì› ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…
    st.markdown("### ğŸ¤– ì§€ì› ì•Œê³ ë¦¬ì¦˜")
    algorithm_descriptions = {
        "RandomForest": "ğŸŒ³ **ëœë¤ í¬ë ˆìŠ¤íŠ¸**: ì—¬ëŸ¬ ê²°ì • íŠ¸ë¦¬ì˜ ì•™ìƒë¸”",
        "GradientBoosting": "ğŸ“ˆ **ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…**: ìˆœì°¨ì  ì•½í•œ í•™ìŠµê¸° ê²°í•©",
        "SVM": "ğŸ¯ **ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ **: ìµœì  ë¶„ë¦¬ ê²½ê³„ ì°¾ê¸°",
        "MLP": "ğŸ§  **ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ **: ì‹¬í™” ì‹ ê²½ë§ êµ¬ì¡°"
    }
    
    for model_name, description in algorithm_descriptions.items():
        if model_name == model_type:
            st.markdown(f"**ì„ íƒë¨**: {description}")
        else:
            st.markdown(description)
    
    # ì‹œìŠ¤í…œ í•™ìŠµ ë°©ì‹ ì„¤ëª…
    st.markdown("### ğŸ“š í•™ìŠµ ì‹œìŠ¤í…œ ì„¤ëª…")
    if data_mode == "Fashion MNIST":
        st.info("ğŸ–¼ï¸ **ì´ë¯¸ì§€ ê¸°ë°˜ ë”¥ëŸ¬ë‹**: Fashion MNISTëŠ” ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ CNNìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.")
    else:
        st.info("ğŸ“Š **ë°ì´í„° ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹**: ìƒí’ˆëª…, ë¸Œëœë“œ, ìƒ‰ìƒ ë“±ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ê²©ëŒ€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ ì„¤ì •
    test_size = st.slider("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (%)", 10, 50, 20, key="sidebar_test_size") / 100
    show_progress = st.checkbox("í•™ìŠµ ì§„í–‰ ìƒí™© í‘œì‹œ", value=True)
    retrain_button = st.button("ëª¨ë¸ ë‹¤ì‹œ í•™ìŠµí•˜ê¸°")
    
    # ëª¨ë¸ í•™ìŠµ ìƒíƒœë¥¼ ì €ì¥í•  ì„¸ì…˜ ìƒíƒœ
    if 'model_trained' not in st.session_state:
        st.session_state.model_trained = False
        
    # ëª¨ë¸ í•™ìŠµ ë©”íŠ¸ë¦­ì„ ì €ì¥í•  ì„¸ì…˜ ìƒíƒœ
    if 'metrics' not in st.session_state:
        st.session_state.metrics = {}
    
    # í˜„ì¬ ì„ íƒëœ ë°ì´í„°ì…‹ í‘œì‹œ
    if 'current_dataset' not in st.session_state:
        st.session_state.current_dataset = "í†µí•© ë°ì´í„°ì…‹"
    
    # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ íƒ€ì…
    if 'current_model_type' not in st.session_state:
        st.session_state.current_model_type = "RandomForest"
    
    # ë°ì´í„°ì…‹ ë˜ëŠ” ëª¨ë¸ì´ ë³€ê²½ë˜ë©´ ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”
    if (st.session_state.current_dataset != dataset_name or 
        st.session_state.current_model_type != model_type):
        st.session_state.model_trained = False
        st.session_state.current_dataset = dataset_name
        st.session_state.current_model_type = model_type

# ë°ì´í„°ì…‹ ë¡œë“œ
if data_mode == "í†µí•© ë°ì´í„°ì…‹ (ê¶Œì¥)":
    df, dataset_info = load_integrated_datasets()
    st.session_state.dataset_info = dataset_info
elif data_mode == "Fashion MNIST":
    dataset_name = "Fashion MNIST ë°ì´í„°ì…‹"
    df = None  # Fashion MNISTëŠ” ë³„ë„ ì²˜ë¦¬
else:
    df = load_single_dataset(dataset_name)
    st.session_state.dataset_info = []

# ë°ì´í„°ì…‹ í†µê³„ ì •ë³´ ì–»ê¸°
if df is not None:
    dataset_stats = get_dataset_stats(df)
else:
    dataset_stats = {}

# ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
if dataset_stats:
    st.sidebar.markdown(f"### í˜„ì¬ ë°ì´í„°ì…‹ ì •ë³´")
    st.sidebar.markdown(f"- ìƒ˜í”Œ ìˆ˜: {dataset_stats['ìƒ˜í”Œ ìˆ˜']:,}ê°œ")
    st.sidebar.markdown(f"- ë¸Œëœë“œ ìˆ˜: {dataset_stats['ë¸Œëœë“œ ìˆ˜']:,}ê°œ")
    st.sidebar.markdown(f"- í‰ê·  ê°€ê²©: â‚¹{dataset_stats['í‰ê·  ê°€ê²©']:.2f}")
    
    if data_mode == "í†µí•© ë°ì´í„°ì…‹ (ê¶Œì¥)" and 'dataset_info' in st.session_state and st.session_state.dataset_info:
        st.sidebar.markdown("### í†µí•© ë°ì´í„° êµ¬ì„±")
        for info in st.session_state.dataset_info:
            percentage = (info['samples'] / sum([d['samples'] for d in st.session_state.dataset_info]) * 100)
            st.sidebar.markdown(f"- {info['name']}: {percentage:.1f}% ({info['samples']:,}ê°œ)")
else:
    st.sidebar.markdown("### ë°ì´í„°ì…‹ ì •ë³´")
    st.sidebar.markdown("ë°ì´í„°ë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...")

# ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ì¬í•™ìŠµ
if data_mode != "Fashion MNIST" and df is not None:
    if not st.session_state.model_trained or retrain_button:
        model, le_brand, le_gender, le_color, tfidf, X_test, y_test, X_meta, y_full, X_train, y_train = prepare_model(df, test_size, show_progress, model_type)

        # ì˜ˆì¸¡ ìˆ˜í–‰ - ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¸ ë°ì´í„° í˜•ì‹ ì‚¬ìš©
        if model_type in ['GradientBoosting', 'SVM', 'MLP']:
            X_test_dense = X_test.toarray() if hasattr(X_test, 'toarray') else X_test
            X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train
            
            if model_type in ['SVM', 'MLP']:
                # SVMê³¼ MLPëŠ” ì •ê·œí™”ëœ ë°ì´í„° ì‚¬ìš©
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train_dense)
                X_test_scaled = scaler.transform(X_test_dense)
                y_pred = model.predict(X_test_scaled)
                y_train_pred = model.predict(X_train_scaled)
            else:
                y_pred = model.predict(X_test_dense)
                y_train_pred = model.predict(X_train_dense)
        else:
            y_pred = model.predict(X_test)
            y_train_pred = model.predict(X_train)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì €ì¥
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        train_accuracy = accuracy_score(y_train, y_train_pred)
        test_accuracy = accuracy_score(y_test, y_pred)
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
        precision_per_class = precision_score(y_test, y_pred, average=None, labels=['Low', 'Medium', 'High'])
        recall_per_class = recall_score(y_test, y_pred, average=None, labels=['Low', 'Medium', 'High'])
        f1_per_class = f1_score(y_test, y_pred, average=None, labels=['Low', 'Medium', 'High'])
        
        st.session_state.metrics = {
            'train_accuracy': train_accuracy * 100,
            'test_accuracy': test_accuracy * 100,
            'train_error_rate': (1 - train_accuracy) * 100,
            'test_error_rate': (1 - test_accuracy) * 100,
            'precision': precision * 100,
            'recall': recall * 100,
            'f1_score': f1 * 100,
            'precision_per_class': precision_per_class * 100,
            'recall_per_class': recall_per_class * 100,
            'f1_per_class': f1_per_class * 100,
            'train_samples': len(y_train),
            'test_samples': len(y_test),
            'train_ratio': (1 - test_size) * 100,
            'test_ratio': test_size * 100,
            'model_type': model_type,
            'data_mode': data_mode,
            'total_samples': len(df) if df is not None else 0,
            'y_pred': y_pred,
            'y_test': y_test
        }
        
        # í†µí•© ë°ì´í„°ì…‹ì˜ ê²½ìš° ë°ì´í„° êµ¬ì„± ì •ë³´ ì¶”ê°€
        if data_mode == "í†µí•© ë°ì´í„°ì…‹ (ê¶Œì¥)" and 'dataset_info' in st.session_state:
            st.session_state.metrics['dataset_composition'] = st.session_state.dataset_info
        
        st.session_state.model_trained = True
    else:
        model, le_brand, le_gender, le_color, tfidf, X_test, y_test, X_meta, y_full, X_train, y_train = prepare_model(df, test_size, False, model_type)
else:
    # Fashion MNISTë‚˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
    model, le_brand, le_gender, le_color, tfidf = None, None, None, None, None
    X_test, y_test, X_meta, y_full, X_train, y_train = None, None, None, None, None, None

# ê¸°ì¡´ ë©”ë‰´ì— ë°ì´í„°ì…‹ ë¹„êµ ë©”ë‰´ ì¶”ê°€
menu = st.selectbox("ğŸ“Œ ë©”ë‰´ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["í•™ìŠµ ë°ì´í„° êµ¬ì„±", "ìƒí’ˆ ì˜ˆì¸¡", "ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„", "ì•Œê³ ë¦¬ì¦˜ ë¹„êµ", "3D ë¶„ì„", "ë§ˆì¼€íŒ… ë° ìš´ì˜ ì „ëµ", "ë°ì´í„° ë³µì¡ì„± ì‹œê°í™”", "í†µí•© AI í•™ìŠµ", "ë¶„ì„ ê¸€"])

if menu == "í•™ìŠµ ë°ì´í„° êµ¬ì„±":
    st.header("ğŸ“Š í•™ìŠµ ë°ì´í„° êµ¬ì„± ë° í†µê³„")
    
    if data_mode == "í†µí•© ë°ì´í„°ì…‹ (ê¶Œì¥)" and df is not None:
        st.success("ğŸ¯ í†µí•© ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì „ì²´ ë°ì´í„°ì…‹ ì •ë³´
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ì´ í•™ìŠµ ìƒ˜í”Œ", f"{len(df):,}ê°œ")
        with col2:
            st.metric("ì´ ë¸Œëœë“œ ìˆ˜", f"{len(df['ProductBrand'].unique()):,}ê°œ")
        with col3:
            st.metric("ë°ì´í„° ì†ŒìŠ¤", f"{len(df['DataSource'].unique())}ê°œ")
        with col4:
            st.metric("í‰ê·  ê°€ê²©", f"â‚¹{df['Price (INR)'].mean():.2f}")
        
        # ë°ì´í„° ì†ŒìŠ¤ë³„ êµ¬ì„±
        st.subheader("ğŸ“ˆ ë°ì´í„° ì†ŒìŠ¤ë³„ êµ¬ì„±")
        
        if 'dataset_info' in st.session_state and st.session_state.dataset_info:
            # ë°ì´í„°ì…‹ ì •ë³´ í…Œì´ë¸”
            dataset_summary = pd.DataFrame(st.session_state.dataset_info)
            dataset_summary['í•™ìŠµë¥  (%)'] = (dataset_summary['samples'] / dataset_summary['samples'].sum() * 100).round(2)
            
            # ì»¬ëŸ¼ëª… í•œêµ­ì–´ë¡œ ë³€ê²½
            dataset_summary_display = dataset_summary.copy()
            dataset_summary_display.columns = ['ë°ì´í„°ì…‹ëª…', 'ìƒ˜í”Œ ìˆ˜', 'ë¸Œëœë“œ ìˆ˜', 'í‰ê·  ê°€ê²©', 'ì†ŒìŠ¤ íŒŒì¼', 'í•™ìŠµë¥  (%)']
            
            st.dataframe(dataset_summary_display, use_container_width=True)
            
            # ì‹œê°í™”
            col1, col2 = st.columns(2)
            
            with col1:
                # ë°ì´í„°ì…‹ë³„ ìƒ˜í”Œ ìˆ˜ íŒŒì´ ì°¨íŠ¸
                fig_pie = px.pie(
                    dataset_summary, 
                    values='samples', 
                    names='name',
                    title='ë°ì´í„°ì…‹ë³„ ìƒ˜í”Œ ë¶„í¬'
                )
                st.plotly_chart(fig_pie)
            
            with col2:
                # ë°ì´í„°ì…‹ë³„ í•™ìŠµë¥  ë§‰ëŒ€ ì°¨íŠ¸
                fig_bar = px.bar(
                    dataset_summary,
                    x='name',
                    y='í•™ìŠµë¥  (%)',
                    title='ë°ì´í„°ì…‹ë³„ í•™ìŠµë¥ ',
                    color='í•™ìŠµë¥  (%)',
                    color_continuous_scale='viridis'
                )
                fig_bar.update_layout(xaxis_tickangle=45)
                st.plotly_chart(fig_bar)
        
        # ë°ì´í„° ì†ŒìŠ¤ë³„ ì„¸ë¶€ ë¶„ì„
        st.subheader("ğŸ” ë°ì´í„° ì†ŒìŠ¤ë³„ ì„¸ë¶€ ë¶„ì„")
        
        # ë°ì´í„° ì†ŒìŠ¤ë³„ í†µê³„
        source_stats = df.groupby('DataSource').agg({
            'ProductName': 'count',
            'Price (INR)': ['mean', 'min', 'max'],
            'ProductBrand': 'nunique',
            'PrimaryColor': 'nunique',
            'Gender': 'nunique'
        }).round(2)
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        source_stats.columns = ['ìƒ˜í”Œ ìˆ˜', 'í‰ê·  ê°€ê²©', 'ìµœì†Œ ê°€ê²©', 'ìµœëŒ€ ê°€ê²©', 'ë¸Œëœë“œ ìˆ˜', 'ìƒ‰ìƒ ìˆ˜', 'ì„±ë³„ ìˆ˜']
        st.dataframe(source_stats, use_container_width=True)
        
        # ê°€ê²©ëŒ€ë³„ ë¶„í¬
        st.subheader("ğŸ’° ê°€ê²©ëŒ€ë³„ ë¶„í¬")
        
        price_dist = df.groupby(['DataSource', 'PriceCategory']).size().unstack(fill_value=0)
        price_dist_pct = price_dist.div(price_dist.sum(axis=1), axis=0) * 100
        
        fig_price = px.bar(
            price_dist_pct,
            title='ë°ì´í„° ì†ŒìŠ¤ë³„ ê°€ê²©ëŒ€ ë¶„í¬ (%)',
            labels={'value': 'ë¹„ìœ¨ (%)', 'index': 'ë°ì´í„° ì†ŒìŠ¤'},
            color_discrete_map={'Low': '#ff9999', 'Medium': '#66b3ff', 'High': '#99ff99'}
        )
        st.plotly_chart(fig_price, use_container_width=True)
        
        # ë¸Œëœë“œ ë¶„í¬
        st.subheader("ğŸ¢ ìƒìœ„ ë¸Œëœë“œ ë¶„í¬")
        
        top_brands = df['ProductBrand'].value_counts().head(15)
        brand_source = df.groupby(['ProductBrand', 'DataSource']).size().unstack(fill_value=0)
        
        fig_brands = px.bar(
            x=top_brands.index,
            y=top_brands.values,
            title='ìƒìœ„ 15ê°œ ë¸Œëœë“œë³„ ìƒí’ˆ ìˆ˜',
            labels={'x': 'ë¸Œëœë“œ', 'y': 'ìƒí’ˆ ìˆ˜'}
        )
        fig_brands.update_layout(xaxis_tickangle=45)
        st.plotly_chart(fig_brands, use_container_width=True)
        
        # í•™ìŠµ í’ˆì§ˆ ì§€í‘œ
        st.subheader("ğŸ“‹ í•™ìŠµ ë°ì´í„° í’ˆì§ˆ ì§€í‘œ")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # ë°ì´í„° ê· í˜•ì„±
            price_balance = df['PriceCategory'].value_counts()
            balance_score = 1 - (price_balance.std() / price_balance.mean())
            st.metric("í´ë˜ìŠ¤ ê· í˜•ì„±", f"{balance_score:.3f}", help="1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê· í˜•ì¡íŒ ë°ì´í„°")
        
        with col2:
            # ë¸Œëœë“œ ë‹¤ì–‘ì„±
            brand_diversity = len(df['ProductBrand'].unique()) / len(df)
            st.metric("ë¸Œëœë“œ ë‹¤ì–‘ì„±", f"{brand_diversity:.3f}", help="ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ë¸Œëœë“œ")
        
        with col3:
            # ê°€ê²© ë¶„í¬ì˜ í‘œì¤€í¸ì°¨
            price_std = df['Price (INR)'].std()
            st.metric("ê°€ê²© ë³€ë™ì„±", f"â‚¹{price_std:.2f}", help="ê°€ê²© ë°ì´í„°ì˜ ë‹¤ì–‘ì„±")
        
        # ê¶Œì¥ì‚¬í•­
        st.subheader("ğŸ’¡ í•™ìŠµ ìµœì í™” ê¶Œì¥ì‚¬í•­")
        
        recommendations = []
        
        if balance_score < 0.8:
            recommendations.append("âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •ì´ë‚˜ ìƒ˜í”Œë§ ê¸°ë²•ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if brand_diversity < 0.1:
            recommendations.append("âš ï¸ ë¸Œëœë“œ ë‹¤ì–‘ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ë” ë§ì€ ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        if len(df) < 1000:
            recommendations.append("âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        if not recommendations:
            st.success("âœ… í˜„ì¬ ë°ì´í„°ì…‹ì€ í•™ìŠµì— ì í•©í•œ í’ˆì§ˆì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤!")
        else:
            for rec in recommendations:
                st.warning(rec)
        
    elif data_mode == "ê°œë³„ ë°ì´í„°ì…‹":
        st.info(f"í˜„ì¬ ì„ íƒëœ ë°ì´í„°ì…‹: **{dataset_name}**")
        
        if df is not None:
            # ê¸°ë³¸ í†µê³„
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ì´ ìƒ˜í”Œ ìˆ˜", f"{len(df):,}ê°œ")
            with col2:
                st.metric("ë¸Œëœë“œ ìˆ˜", f"{len(df['ProductBrand'].unique()):,}ê°œ")
            with col3:
                st.metric("í‰ê·  ê°€ê²©", f"â‚¹{df['Price (INR)'].mean():.2f}")
            with col4:
                learning_rate = 100.0  # ê°œë³„ ë°ì´í„°ì…‹ì€ 100%
                st.metric("í•™ìŠµë¥ ", f"{learning_rate:.1f}%")
            
            # ê°€ê²©ëŒ€ ë¶„í¬
            st.subheader("ğŸ’° ê°€ê²©ëŒ€ ë¶„í¬")
            price_counts = df['PriceCategory'].value_counts()
            fig = px.pie(values=price_counts.values, names=price_counts.index, title='ê°€ê²©ëŒ€ë³„ ë¶„í¬')
            st.plotly_chart(fig)
            
        else:
            st.error("ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    elif data_mode == "Fashion MNIST":
        st.info("Fashion MNIST ë°ì´í„°ì…‹ ëª¨ë“œ")
        st.markdown("""
        ### Fashion MNIST ë°ì´í„°ì…‹ ì •ë³´
        - **í•™ìŠµ ë°ì´í„°**: 60,000ê°œ ì´ë¯¸ì§€
        - **í…ŒìŠ¤íŠ¸ ë°ì´í„°**: 10,000ê°œ ì´ë¯¸ì§€  
        - **í´ë˜ìŠ¤ ìˆ˜**: 10ê°œ (ì˜ë¥˜ ì¹´í…Œê³ ë¦¬)
        - **ì´ë¯¸ì§€ í¬ê¸°**: 28x28 í”½ì…€
        - **í•™ìŠµë¥ **: 100% (ì „ì²´ ë°ì´í„° ì‚¬ìš©)
        """)

elif menu == "ìƒí’ˆ ì˜ˆì¸¡":
    st.header("ğŸ›ï¸ ê°œë³„ ìƒí’ˆ ê°€ê²©ëŒ€ ì˜ˆì¸¡")
    
    if data_mode == "Fashion MNIST":
        st.warning("Fashion MNIST ëª¨ë“œì—ì„œëŠ” ìƒí’ˆ ì˜ˆì¸¡ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°ì´í„°ì…‹ ëª¨ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    elif df is None or model is None:
        st.warning("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
    else:
        name = st.text_input("ìƒí’ˆëª… ì…ë ¥ (ì˜ˆ: Slim Fit Checked Shirt)")
        brand = st.selectbox("ë¸Œëœë“œ ì„ íƒ", df['ProductBrand'].unique())
        gender = st.selectbox("ì„±ë³„", df['Gender'].unique())
        color = st.selectbox("ê¸°ë³¸ ìƒ‰ìƒ", df['PrimaryColor'].unique())
        num_images = st.slider("ì´ë¯¸ì§€ ê°œìˆ˜", 1, 10, 1)

    if st.button("ê°€ê²©ëŒ€ ì˜ˆì¸¡"):
        try:
            prediction, proba = classify_product_proba(model, le_brand, le_gender, le_color, tfidf, name, brand, gender, color, num_images)
            st.subheader(f"âœ… ì˜ˆì¸¡ ê²°ê³¼: {prediction} ê°€ê²©ëŒ€")

            st.write("### ğŸ”¢ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬")
            fig, ax = plt.subplots(figsize=(8, 3))
            colors = ['#ff9999', '#66b3ff', '#99ff99']
            ax.barh(model.classes_, proba, color=colors)
            ax.set_xlim(0, 1)
            ax.set_xlabel('í™•ë¥ ')
            ax.set_title('í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ í™•ë¥ ')
            
            for i, v in enumerate(proba):
                ax.text(v + 0.01, i, f"{v:.2%}", va='center')
                
            st.pyplot(fig)

            if prediction == 'Low':
                st.info("ğŸ’¡ ì €ê°€ ìƒí’ˆ: ê°€ê²© ë¯¼ê°í˜• ê³ ê° ëŒ€ìƒ í• ì¸ ë§ˆì¼€íŒ…")
            elif prediction == 'High':
                st.warning("ğŸ’¡ ê³ ê°€ ìƒí’ˆ: ê³ ê¸‰ ë¸Œëœë“œ ì¤‘ì‹¬ ê´‘ê³  ë° ë¦¬ë·° ì „ëµ")
            else:
                st.success("ğŸ’¡ ì¤‘ê°„ ê°€ê²©ëŒ€: ë‹¤ì–‘í•œ ê°€ê²© í…ŒìŠ¤íŠ¸ ë° ì¶”ì²œ ìµœì í™”")
                    
                # í†µí•© ë°ì´í„°ì…‹ì¸ ê²½ìš° ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ë„ í‘œì‹œ
                if data_mode == "í†µí•© ë°ì´í„°ì…‹ (ê¶Œì¥)":
                    st.write("### ğŸ“Š í•™ìŠµ ë°ì´í„° êµ¬ì„±")
                    if 'dataset_composition' in st.session_state.metrics:
                        for info in st.session_state.metrics['dataset_composition']:
                            percentage = (info['samples'] / st.session_state.metrics['total_samples'] * 100)
                            st.write(f"- {info['name']}: {percentage:.1f}% ê¸°ì—¬")
                            
        except Exception as e:
            st.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

elif menu == "ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„":
    st.header("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë° í‰ê°€ ë¦¬í¬íŠ¸")
    
    if 'metrics' not in st.session_state or not st.session_state.metrics:
        st.warning("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
    else:
        # í˜„ì¬ ì‚¬ìš©ëœ ëª¨ë¸ í‘œì‹œ
        current_model = st.session_state.metrics.get('model_type', 'RandomForest')
        current_data_mode = st.session_state.metrics.get('data_mode', 'ê°œë³„ ë°ì´í„°ì…‹')
        
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"ğŸ¤– ì‚¬ìš©ëœ ì•Œê³ ë¦¬ì¦˜: **{current_model}**")
        with col2:
            st.info(f"ğŸ“Š ë°ì´í„° ëª¨ë“œ: **{current_data_mode}**")
        
        # í†µí•© ë°ì´í„°ì…‹ì¸ ê²½ìš° í•™ìŠµ ë°ì´í„° êµ¬ì„± í‘œì‹œ
        if current_data_mode == "í†µí•© ë°ì´í„°ì…‹ (ê¶Œì¥)" and 'dataset_composition' in st.session_state.metrics:
            st.subheader("ğŸ“ˆ í•™ìŠµ ë°ì´í„° êµ¬ì„±")
            composition_data = []
            total_samples = st.session_state.metrics['total_samples']
            
            for info in st.session_state.metrics['dataset_composition']:
                percentage = (info['samples'] / total_samples * 100)
                composition_data.append({
                    'ë°ì´í„°ì…‹': info['name'],
                    'ìƒ˜í”Œ ìˆ˜': f"{info['samples']:,}ê°œ",
                    'í•™ìŠµë¥ ': f"{percentage:.1f}%",
                    'í‰ê·  ê°€ê²©': f"â‚¹{info['avg_price']:.2f}"
                })
            
            composition_df = pd.DataFrame(composition_data)
            st.dataframe(composition_df, use_container_width=True)
        
        # ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ í‘œì‹œ
        st.subheader("ğŸ¯ ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("í…ŒìŠ¤íŠ¸ ì •í™•ë„", f"{st.session_state.metrics['test_accuracy']:.2f}%")
            st.metric("í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ìœ¨", f"{st.session_state.metrics['test_error_rate']:.2f}%", delta=f"-{st.session_state.metrics['test_accuracy']:.1f}%")
        
        with col2:
            st.metric("ì •ë°€ë„ (Precision)", f"{st.session_state.metrics['precision']:.2f}%")
            st.metric("ì¬í˜„ìœ¨ (Recall)", f"{st.session_state.metrics['recall']:.2f}%")
        
        with col3:
            st.metric("F1 ì ìˆ˜", f"{st.session_state.metrics['f1_score']:.2f}%")
            overfitting = st.session_state.metrics['train_accuracy'] - st.session_state.metrics['test_accuracy']
            st.metric("ê³¼ì í•©ë„", f"{overfitting:.2f}%", delta=f"{overfitting:.1f}%")
        
        with col4:
            st.metric("í•™ìŠµ ì •í™•ë„", f"{st.session_state.metrics['train_accuracy']:.2f}%")
            st.metric("í•™ìŠµ ì˜¤ì°¨ìœ¨", f"{st.session_state.metrics['train_error_rate']:.2f}%")
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ
        st.subheader("ğŸ“Š í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ")
        
        class_metrics_df = pd.DataFrame({
            'ê°€ê²©ëŒ€': ['Low (ì €ê°€)', 'Medium (ì¤‘ê°€)', 'High (ê³ ê°€)'],
            'ì •ë°€ë„ (%)': st.session_state.metrics['precision_per_class'],
            'ì¬í˜„ìœ¨ (%)': st.session_state.metrics['recall_per_class'],
            'F1 ì ìˆ˜ (%)': st.session_state.metrics['f1_per_class']
        })
        
        # ìŠ¤íƒ€ì¼ë§ëœ í…Œì´ë¸”ë¡œ í‘œì‹œ
        st.dataframe(
            class_metrics_df.style.format({
                'ì •ë°€ë„ (%)': '{:.2f}',
                'ì¬í˜„ìœ¨ (%)': '{:.2f}',
                'F1 ì ìˆ˜ (%)': '{:.2f}'
            }).background_gradient(cmap='RdYlGn', vmin=0, vmax=100),
            use_container_width=True
        )
        
        # ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™”
        fig_metrics = px.bar(
            class_metrics_df.melt(id_vars='ê°€ê²©ëŒ€', var_name='ì§€í‘œ', value_name='ê°’'),
            x='ê°€ê²©ëŒ€',
            y='ê°’',
            color='ì§€í‘œ',
            title='í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ ë¹„êµ',
            barmode='group'
        )
        fig_metrics.update_layout(yaxis_title='ì„±ëŠ¥ (%)', xaxis_title='ê°€ê²©ëŒ€')
        st.plotly_chart(fig_metrics, use_container_width=True)
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë³´
        st.subheader("ğŸ“ˆ ë°ì´í„° ë¶„í•  ì •ë³´")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### í•™ìŠµ ë°ì´í„°")
            st.metric("í•™ìŠµ ë°ì´í„° ë¹„ìœ¨", f"{st.session_state.metrics['train_ratio']:.1f}%")
            st.metric("í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ìˆ˜", f"{st.session_state.metrics['train_samples']:,}")
            st.metric("í•™ìŠµ ë°ì´í„° ì •í™•ë„", f"{st.session_state.metrics['train_accuracy']:.2f}%")
            
        with col2:
            st.markdown("### í…ŒìŠ¤íŠ¸ ë°ì´í„°")
            st.metric("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨", f"{st.session_state.metrics['test_ratio']:.1f}%")
            st.metric("í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œ ìˆ˜", f"{st.session_state.metrics['test_samples']:,}")
            st.metric("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„", f"{st.session_state.metrics['test_accuracy']:.2f}%")
    
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ ê·¸ë˜í”„
        st.subheader("ğŸ“Š í•™ìŠµ vs í…ŒìŠ¤íŠ¸ ì •í™•ë„")
        fig, ax = plt.subplots(figsize=(8, 4))
        bars = ax.bar(['í•™ìŠµ ì •í™•ë„', 'í…ŒìŠ¤íŠ¸ ì •í™•ë„'], 
                    [st.session_state.metrics['train_accuracy'], st.session_state.metrics['test_accuracy']], 
                    color=['#5cb85c', '#5bc0de'])
        ax.set_ylim(0, 100)
        ax.set_ylabel('ì •í™•ë„ (%)')
        ax.set_title('í•™ìŠµ vs í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ')
        
        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 1, f'{height:.2f}%', 
                    ha='center', va='bottom')
        
        st.pyplot(fig)
        
        if model is not None and X_test is not None:
            y_pred = model.predict(X_test)

    st.markdown("#### âœ… Confusion Matrix")
    cm = confusion_matrix(y_test, y_pred, labels=['Low', 'Medium', 'High'])
    fig, ax = plt.subplots(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])
    plt.xlabel("ì˜ˆì¸¡ ê°’")
    plt.ylabel("ì‹¤ì œ ê°’")
    st.pyplot(fig)

    st.markdown("#### âœ… ë¶„ë¥˜ ì„±ëŠ¥ ë³´ê³ ì„œ")
    report = classification_report(y_test, y_pred, output_dict=True)
    st.dataframe(pd.DataFrame(report).transpose())
    
    if hasattr(model, 'feature_importances_'):
        st.markdown("#### âœ… íŠ¹ì„± ì¤‘ìš”ë„")
        feature_importance = model.feature_importances_
        # TFIDF íŠ¹ì„±ê³¼ ë©”íƒ€ë°ì´í„° íŠ¹ì„±ì„ êµ¬ë¶„
        n_tfidf_features = tfidf.get_feature_names_out().shape[0]
        meta_features = ['ë¸Œëœë“œ', 'ì„±ë³„', 'ìƒ‰ìƒ', 'ì´ë¯¸ì§€ ìˆ˜']
        
        # ë©”íƒ€ë°ì´í„° íŠ¹ì„± ì¤‘ìš”ë„ë§Œ ì‹œê°í™”
        meta_importance = feature_importance[-len(meta_features):]
        
        fig, ax = plt.subplots(figsize=(8, 4))
        bars = ax.barh(meta_features, meta_importance, color='skyblue')
        ax.set_xlabel('ì¤‘ìš”ë„')
        ax.set_title('ë©”íƒ€ë°ì´í„° íŠ¹ì„± ì¤‘ìš”ë„')
        
        # ë§‰ëŒ€ ëì— ê°’ í‘œì‹œ
        for i, v in enumerate(meta_importance):
            ax.text(v + 0.01, i, f"{v:.4f}", va='center')
        st.pyplot(fig)
        
        # ì„±ëŠ¥ ì§€í‘œ ì„¤ëª…
        with st.expander("ğŸ“š ì„±ëŠ¥ ì§€í‘œ ì„¤ëª…"):
            st.markdown("""
            ### ğŸ¯ ë¨¸ì‹ ëŸ¬ë‹ ì„±ëŠ¥ ì§€í‘œ ì„¤ëª…
            
            #### 1. **ì •í™•ë„ (Accuracy)**
            - **ì˜ë¯¸**: ì „ì²´ ì˜ˆì¸¡ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
            - **ê³„ì‚°**: (ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ ìˆ˜) / (ì „ì²´ ì˜ˆì¸¡ ìˆ˜) Ã— 100
            - **í•´ì„**: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (100%ê°€ ìµœê³ )
            
            #### 2. **ì˜¤ì°¨ìœ¨ (Error Rate)**
            - **ì˜ë¯¸**: ì „ì²´ ì˜ˆì¸¡ ì¤‘ ì˜ëª» ì˜ˆì¸¡í•œ ë¹„ìœ¨
            - **ê³„ì‚°**: 100% - ì •í™•ë„
            - **í•´ì„**: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (0%ê°€ ìµœê³ )
            
            #### 3. **ì •ë°€ë„ (Precision)**
            - **ì˜ë¯¸**: íŠ¹ì • í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œë¡œ ê·¸ í´ë˜ìŠ¤ì¸ ë¹„ìœ¨
            - **ì˜ˆì‹œ**: "High"ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œë¡œ "High"ì¸ ë¹„ìœ¨
            - **í•´ì„**: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (ê±°ì§“ ì–‘ì„±ì´ ì ìŒ)
            
            #### 4. **ì¬í˜„ìœ¨ (Recall)**
            - **ì˜ë¯¸**: ì‹¤ì œ íŠ¹ì • í´ë˜ìŠ¤ì¸ ê²ƒ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
            - **ì˜ˆì‹œ**: ì‹¤ì œ "High"ì¸ ê²ƒ ì¤‘ "High"ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
            - **í•´ì„**: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ì ìŒ)
            
            #### 5. **F1 ì ìˆ˜ (F1 Score)**
            - **ì˜ë¯¸**: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· 
            - **ê³„ì‚°**: 2 Ã— (ì •ë°€ë„ Ã— ì¬í˜„ìœ¨) / (ì •ë°€ë„ + ì¬í˜„ìœ¨)
            - **í•´ì„**: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ê· í˜•ì„ ë‚˜íƒ€ëƒ„
            
            #### 6. **ê³¼ì í•©ë„ (Overfitting Degree)**
            - **ì˜ë¯¸**: í•™ìŠµ ì •í™•ë„ì™€ í…ŒìŠ¤íŠ¸ ì •í™•ë„ì˜ ì°¨ì´
            - **ê³„ì‚°**: í•™ìŠµ ì •í™•ë„ - í…ŒìŠ¤íŠ¸ ì •í™•ë„
            - **í•´ì„**: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ìŒ)
            
            ### ğŸ’¡ í•´ì„ ê°€ì´ë“œ
            - **ì •í™•ë„ > 90%**: ë§¤ìš° ìš°ìˆ˜í•œ ì„±ëŠ¥
            - **ì •í™•ë„ 80-90%**: ì¢‹ì€ ì„±ëŠ¥
            - **ì •í™•ë„ 70-80%**: ë³´í†µ ì„±ëŠ¥
            - **ì •í™•ë„ < 70%**: ê°œì„  í•„ìš”
            
            - **ê³¼ì í•©ë„ < 5%**: ì¢‹ì€ ì¼ë°˜í™”
            - **ê³¼ì í•©ë„ 5-10%**: ì•½ê°„ì˜ ê³¼ì í•©
            - **ê³¼ì í•©ë„ > 10%**: ì‹¬í•œ ê³¼ì í•© (ëª¨ë¸ ê°œì„  í•„ìš”)
            """)
        
        # ROC ê³¡ì„  ì¶”ê°€ (ì´ì§„ ë¶„ë¥˜ê°€ ì•„ë‹ˆë¯€ë¡œ í´ë˜ìŠ¤ë³„ë¡œ)
        if hasattr(model, 'predict_proba') and 'y_test' in st.session_state.metrics:
            st.subheader("ğŸ“ˆ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬")
            
            y_test = st.session_state.metrics['y_test']
            y_pred = st.session_state.metrics['y_pred']
            
            # ì˜ˆì¸¡ í™•ë¥  ê°€ì ¸ì˜¤ê¸°
            y_proba = model.predict_proba(X_test)
            
            # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))
            classes = ['Low', 'Medium', 'High']
            
            for idx, (ax, class_name) in enumerate(zip(axes, classes)):
                # ì‹¤ì œ í´ë˜ìŠ¤ì™€ ì˜ˆì¸¡ í´ë˜ìŠ¤ë³„ë¡œ í™•ë¥  ë¶„í¬ í‘œì‹œ
                for actual_class in classes:
                    mask = y_test == actual_class
                    if mask.sum() > 0:
                        ax.hist(y_proba[mask, idx], bins=20, alpha=0.5, label=f'ì‹¤ì œ: {actual_class}')
                
                ax.set_xlabel(f'{class_name} ì˜ˆì¸¡ í™•ë¥ ')
                ax.set_ylabel('ë¹ˆë„')
                ax.set_title(f'{class_name} í´ë˜ìŠ¤ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬')
                ax.legend()
                ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            st.pyplot(fig)

elif menu == "ì•Œê³ ë¦¬ì¦˜ ë¹„êµ":
    st.header("ğŸ”¬ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ")
    
    st.markdown("""
    ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    # ë¹„êµí•  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
    algorithms_to_compare = st.multiselect(
        "ë¹„êµí•  ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•˜ì„¸ìš”",
        ["RandomForest", "XGBoost", "LightGBM", "CatBoost", "GradientBoosting", "SVM", "MLP"],
        default=["RandomForest", "XGBoost"] if XGBOOST_AVAILABLE else ["RandomForest", "GradientBoosting"]
    )
    
    if len(algorithms_to_compare) < 2:
        st.warning("ë¹„êµë¥¼ ìœ„í•´ ìµœì†Œ 2ê°œì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        if st.button("ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹œì‘", type="primary"):
            results = {}
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, algorithm in enumerate(algorithms_to_compare):
                status_text.text(f"{algorithm} í•™ìŠµ ì¤‘... ({i+1}/{len(algorithms_to_compare)})")
                progress_bar.progress((i+1) / len(algorithms_to_compare))
                
                # ê° ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ
                try:
                    model, _, _, _, _, X_test_algo, y_test_algo, _, _, X_train_algo, y_train_algo = prepare_model(
                        df, test_size, False, algorithm
                    )
                    
                    # ì˜ˆì¸¡ ìˆ˜í–‰
                    if algorithm in ['XGBoost', 'LightGBM', 'CatBoost', 'GradientBoosting', 'SVM', 'MLP']:
                        X_test_dense = X_test_algo.toarray() if hasattr(X_test_algo, 'toarray') else X_test_algo
                        X_train_dense = X_train_algo.toarray() if hasattr(X_train_algo, 'toarray') else X_train_algo
                        
                        if algorithm in ['SVM', 'MLP']:
                            scaler = StandardScaler()
                            X_train_scaled = scaler.fit_transform(X_train_dense)
                            X_test_scaled = scaler.transform(X_test_dense)
                            y_pred = model.predict(X_test_scaled)
                            y_train_pred = model.predict(X_train_scaled)
                        else:
                            y_pred = model.predict(X_test_dense)
                            y_train_pred = model.predict(X_train_dense)
                    else:
                        y_pred = model.predict(X_test_algo)
                        y_train_pred = model.predict(X_train_algo)
                    
                    # ì„±ëŠ¥ ê³„ì‚°
                    from sklearn.metrics import precision_score, recall_score, f1_score
                    
                    train_acc = accuracy_score(y_train_algo, y_train_pred)
                    test_acc = accuracy_score(y_test_algo, y_pred)
                    
                    # ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
                    precision = precision_score(y_test_algo, y_pred, average='weighted')
                    recall = recall_score(y_test_algo, y_pred, average='weighted')
                    f1 = f1_score(y_test_algo, y_pred, average='weighted')
                    
                    results[algorithm] = {
                        'train_accuracy': train_acc * 100,
                        'test_accuracy': test_acc * 100,
                        'train_error_rate': (1 - train_acc) * 100,
                        'test_error_rate': (1 - test_acc) * 100,
                        'precision': precision * 100,
                        'recall': recall * 100,
                        'f1_score': f1 * 100,
                        'overfitting': (train_acc - test_acc) * 100,
                        'model': model,
                        'predictions': y_pred,
                        'true_labels': y_test_algo
                    }
                    
                except Exception as e:
                    st.error(f"{algorithm} í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            status_text.text("ë¹„êµ ì™„ë£Œ!")
            progress_bar.progress(1.0)
            
            # ê²°ê³¼ í‘œì‹œ
            if results:
                # ì„±ëŠ¥ ë¹„êµ í‘œ
                st.subheader("ğŸ“Š ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ")
                
                comparison_df = pd.DataFrame({
                    'ì•Œê³ ë¦¬ì¦˜': list(results.keys()),
                    'í•™ìŠµ ì •í™•ë„ (%)': [results[algo]['train_accuracy'] for algo in results.keys()],
                    'í…ŒìŠ¤íŠ¸ ì •í™•ë„ (%)': [results[algo]['test_accuracy'] for algo in results.keys()],
                    'í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ìœ¨ (%)': [results[algo]['test_error_rate'] for algo in results.keys()],
                    'ì •ë°€ë„ (%)': [results[algo]['precision'] for algo in results.keys()],
                    'ì¬í˜„ìœ¨ (%)': [results[algo]['recall'] for algo in results.keys()],
                    'F1 ì ìˆ˜ (%)': [results[algo]['f1_score'] for algo in results.keys()],
                    'ê³¼ì í•©ë„ (%)': [results[algo]['overfitting'] for algo in results.keys()]
                })
                
                # ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜ í‘œì‹œ
                best_algo = comparison_df.loc[comparison_df['í…ŒìŠ¤íŠ¸ ì •í™•ë„ (%)'].idxmax(), 'ì•Œê³ ë¦¬ì¦˜']
                st.success(f"ğŸ† ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜: **{best_algo}** (í…ŒìŠ¤íŠ¸ ì •í™•ë„: {comparison_df.loc[comparison_df['í…ŒìŠ¤íŠ¸ ì •í™•ë„ (%)'].idxmax(), 'í…ŒìŠ¤íŠ¸ ì •í™•ë„ (%)']:.2f}%)")
                
                st.table(comparison_df)
                
                # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
                st.subheader("ğŸ“Š ì¢…í•© ì„±ëŠ¥ ë¹„êµ")
                
                # ì •í™•ë„ ë° ì˜¤ì°¨ìœ¨ ë¹„êµ
                fig1 = px.bar(
                    comparison_df, 
                    x='ì•Œê³ ë¦¬ì¦˜', 
                    y=['í…ŒìŠ¤íŠ¸ ì •í™•ë„ (%)', 'í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ìœ¨ (%)'],
                    barmode='group',
                    title='ì•Œê³ ë¦¬ì¦˜ë³„ ì •í™•ë„ vs ì˜¤ì°¨ìœ¨',
                    color_discrete_sequence=['#5cb85c', '#d9534f']
                )
                st.plotly_chart(fig1, use_container_width=True)
                
                # ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ ë¹„êµ
                fig2 = px.bar(
                    comparison_df,
                    x='ì•Œê³ ë¦¬ì¦˜',
                    y=['ì •ë°€ë„ (%)', 'ì¬í˜„ìœ¨ (%)', 'F1 ì ìˆ˜ (%)'],
                    barmode='group',
                    title='ì•Œê³ ë¦¬ì¦˜ë³„ ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜',
                    color_discrete_sequence=['#f0ad4e', '#5bc0de', '#5cb85c']
                )
                st.plotly_chart(fig2, use_container_width=True)
                
                # ë ˆì´ë” ì°¨íŠ¸ë¡œ ì¢…í•© ë¹„êµ
                st.subheader("ğŸ¯ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸")
                
                # ê° ì•Œê³ ë¦¬ì¦˜ë³„ ë ˆì´ë” ì°¨íŠ¸
                metrics_for_radar = ['í…ŒìŠ¤íŠ¸ ì •í™•ë„ (%)', 'ì •ë°€ë„ (%)', 'ì¬í˜„ìœ¨ (%)', 'F1 ì ìˆ˜ (%)']
                
                fig_radar = go.Figure()
                
                for algo in results.keys():
                    values = [results[algo]['test_accuracy'], 
                             results[algo]['precision'],
                             results[algo]['recall'],
                             results[algo]['f1_score']]
                    
                    fig_radar.add_trace(go.Scatterpolar(
                        r=values,
                        theta=metrics_for_radar,
                        fill='toself',
                        name=algo
                    ))
                
                fig_radar.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 100]
                        )),
                    showlegend=True,
                    title="ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ì§€í‘œ ë¹„êµ"
                )
                
                st.plotly_chart(fig_radar, use_container_width=True)
                
                # ê³¼ì í•© ë¶„ì„
                st.subheader("ğŸ“ˆ ê³¼ì í•© ë¶„ì„")
                fig2 = px.bar(
                    comparison_df,
                    x='ì•Œê³ ë¦¬ì¦˜',
                    y='ê³¼ì í•©ë„ (%)',
                    title='ì•Œê³ ë¦¬ì¦˜ë³„ ê³¼ì í•© ì •ë„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)',
                    color='ê³¼ì í•©ë„ (%)',
                    color_continuous_scale='RdYlGn_r'
                )
                st.plotly_chart(fig2, use_container_width=True)
                
                # ìƒì„¸ ë¶„ì„
                with st.expander("ğŸ” ìƒì„¸ ë¶„ì„ ë³´ê¸°"):
                    selected_algo = st.selectbox("ë¶„ì„í•  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ", list(results.keys()))
                    
                    if selected_algo in results:
                        st.write(f"### {selected_algo} ìƒì„¸ ë¶„ì„")
                        
                        # Confusion Matrix
                        cm = confusion_matrix(results[selected_algo]['true_labels'], results[selected_algo]['predictions'])
                        fig, ax = plt.subplots(figsize=(8, 6))
                        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                                   xticklabels=['Low', 'Medium', 'High'], 
                                   yticklabels=['Low', 'Medium', 'High'])
                        plt.title(f'{selected_algo} - Confusion Matrix')
                        plt.xlabel("ì˜ˆì¸¡ ê°’")
                        plt.ylabel("ì‹¤ì œ ê°’")
                        st.pyplot(fig)
                        
                        # ë¶„ë¥˜ ë³´ê³ ì„œ
                        report = classification_report(
                            results[selected_algo]['true_labels'], 
                            results[selected_algo]['predictions'], 
                            output_dict=True
                        )
                        report_df = pd.DataFrame(report).transpose()
                        st.dataframe(report_df)
                
                # ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„± ë¹„êµ
                st.subheader("âš¡ ì•Œê³ ë¦¬ì¦˜ íŠ¹ì„± ë¹„êµ")
                
                algo_characteristics = {
                    'RandomForest': {'ì†ë„': 'ì¤‘ê°„', 'í•´ì„ì„±': 'ë†’ìŒ', 'ê³¼ì í•© ì €í•­ì„±': 'ë†’ìŒ', 'ë©”ëª¨ë¦¬ ì‚¬ìš©': 'ì¤‘ê°„'},
                    'XGBoost': {'ì†ë„': 'ë¹ ë¦„', 'í•´ì„ì„±': 'ì¤‘ê°„', 'ê³¼ì í•© ì €í•­ì„±': 'ë†’ìŒ', 'ë©”ëª¨ë¦¬ ì‚¬ìš©': 'ë‚®ìŒ'},
                    'LightGBM': {'ì†ë„': 'ë§¤ìš° ë¹ ë¦„', 'í•´ì„ì„±': 'ì¤‘ê°„', 'ê³¼ì í•© ì €í•­ì„±': 'ë†’ìŒ', 'ë©”ëª¨ë¦¬ ì‚¬ìš©': 'ë§¤ìš° ë‚®ìŒ'},
                    'CatBoost': {'ì†ë„': 'ì¤‘ê°„', 'í•´ì„ì„±': 'ë†’ìŒ', 'ê³¼ì í•© ì €í•­ì„±': 'ë§¤ìš° ë†’ìŒ', 'ë©”ëª¨ë¦¬ ì‚¬ìš©': 'ì¤‘ê°„'},
                    'GradientBoosting': {'ì†ë„': 'ëŠë¦¼', 'í•´ì„ì„±': 'ì¤‘ê°„', 'ê³¼ì í•© ì €í•­ì„±': 'ì¤‘ê°„', 'ë©”ëª¨ë¦¬ ì‚¬ìš©': 'ì¤‘ê°„'},
                    'SVM': {'ì†ë„': 'ëŠë¦¼', 'í•´ì„ì„±': 'ë‚®ìŒ', 'ê³¼ì í•© ì €í•­ì„±': 'ë†’ìŒ', 'ë©”ëª¨ë¦¬ ì‚¬ìš©': 'ë†’ìŒ'},
                    'MLP': {'ì†ë„': 'ì¤‘ê°„', 'í•´ì„ì„±': 'ë‚®ìŒ', 'ê³¼ì í•© ì €í•­ì„±': 'ë‚®ìŒ', 'ë©”ëª¨ë¦¬ ì‚¬ìš©': 'ë†’ìŒ'}
                }
                
                char_df = pd.DataFrame(algo_characteristics).T
                char_df = char_df.loc[char_df.index.intersection(algorithms_to_compare)]
                st.table(char_df)
                
                # ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤
                st.subheader("ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì²œ")
                
                scenarios = {
                    'RandomForest': 'â­ ì•ˆì •ì ì´ê³  í•´ì„ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ í•„ìš”í•œ ê²½ìš°',
                    'XGBoost': 'ğŸš€ ë†’ì€ ì„±ëŠ¥ê³¼ ë¹ ë¥¸ í•™ìŠµì´ í•„ìš”í•œ ê²½ìš°',
                    'LightGBM': 'âš¡ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì™€ ë¹ ë¥¸ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°',
                    'CatBoost': 'ğŸ”’ ê³¼ì í•©ì— ê°•í•˜ê³  ì•ˆì •ì ì¸ ì„±ëŠ¥ì´ í•„ìš”í•œ ê²½ìš°',
                    'GradientBoosting': 'ğŸ“š ì „í†µì ì¸ ë¶€ìŠ¤íŒ… ë°©ë²•ì„ ì„ í˜¸í•˜ëŠ” ê²½ìš°',
                    'SVM': 'ğŸ¯ ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ê°•ë ¥í•œ ë¶„ë¥˜ê°€ í•„ìš”í•œ ê²½ìš°',
                    'MLP': 'ğŸ§  ë³µì¡í•œ ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµì´ í•„ìš”í•œ ê²½ìš°'
                }
                
                for algo in algorithms_to_compare:
                    if algo in scenarios:
                        st.info(f"**{algo}**: {scenarios[algo]}")

elif menu == "3D ë¶„ì„":
    st.header("ğŸ§  ë¸Œëœë“œ / ì´ë¯¸ì§€ ìˆ˜ / ê°€ê²© ì‹œê°í™” (3D)")
    df_plot = df.copy()
    df_plot['PriceCategory'] = y_full
    fig3d = px.scatter_3d(df_plot, x='ProductBrand', y='NumImages', z='Price (INR)', color='PriceCategory', title='ë¸Œëœë“œ vs ì´ë¯¸ì§€ìˆ˜ vs ê°€ê²©ëŒ€ (3D)')
    st.plotly_chart(fig3d)

elif menu == "ë§ˆì¼€íŒ… ë° ìš´ì˜ ì „ëµ":
    st.header("ğŸ“ˆ ë§ˆì¼€íŒ… ë° ìš´ì˜ ì¸ì‚¬ì´íŠ¸")
    st.write("### ğŸ’° ìƒìœ„ ë§¤ì¶œ ì˜ˆìƒ ìƒí’ˆ")
    top_rev = df.sort_values(by='ExpectedRevenue', ascending=False).head(10)
    st.dataframe(top_rev[['ProductName', 'Price (INR)', 'ExpectedCustomers', 'ExpectedSales', 'ExpectedRevenue']])

    fig_rev = px.bar(top_rev, x='ProductName', y='ExpectedRevenue', title='ì˜ˆìƒ ë§¤ì¶œ ìƒìœ„ 10ê°œ ì œí’ˆ', labels={'ExpectedRevenue': 'ì˜ˆìƒ ë§¤ì¶œ'})
    st.plotly_chart(fig_rev)

    fig_pie = px.pie(df, names='PriceCategory', values='ExpectedSales', title='ê°€ê²©ëŒ€ë³„ ì˜ˆìƒ íŒë§¤ ë¹„ìœ¨')
    st.plotly_chart(fig_pie)

    st.write("### ğŸ§® ë§¤ì¶œ ì‚°ì¶œ ë°©ì‹:")
    st.code("ExpectedRevenue = ì˜ˆìƒê³ ê°ìˆ˜ Ã— ì „í™˜ìœ¨ Ã— ê°€ê²©")

    st.success("ì´ ì •ë³´ëŠ” ì‹¤ì œ ìƒí’ˆ ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ ë° ë§¤ì¶œ ì „ëµ ìˆ˜ë¦½ ê·¼ê±°ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

elif menu == "ë°ì´í„°ì…‹ ë¹„êµ ë¶„ì„":
    st.header("ğŸ“Š ë°ì´í„°ì…‹ ë¹„êµ ë¶„ì„")
    
    st.markdown("""
    ì—¬ëŸ¬ íŒ¨ì…˜ ë°ì´í„°ì…‹ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ê° ë°ì´í„°ì…‹ì˜ íŠ¹ì„±ê³¼ ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ ì°¨ì´ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤.
    ì‚¬ì´ë“œë°”ì—ì„œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ê³  'ëª¨ë¸ ë‹¤ì‹œ í•™ìŠµí•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ í•´ë‹¹ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•œ í›„,
    ì´ í˜ì´ì§€ì—ì„œ ê²°ê³¼ë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    # í˜„ì¬ ì„ íƒëœ ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
    st.subheader(f"í˜„ì¬ ì„ íƒëœ ë°ì´í„°ì…‹: {dataset_name}")
    
    # ë°ì´í„°ì…‹ í†µê³„ ì •ë³´ ìƒì„¸ í‘œì‹œ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´")
        stats_df = pd.DataFrame({
            'í•­ëª©': [
                'ìƒ˜í”Œ ìˆ˜', 'ë¸Œëœë“œ ìˆ˜', 'ì„±ë³„ ì¢…ë¥˜', 'ìƒ‰ìƒ ì¢…ë¥˜',
                'í‰ê·  ê°€ê²©', 'ìµœì†Œ ê°€ê²©', 'ìµœëŒ€ ê°€ê²©'
            ],
            'ê°’': [
                f"{dataset_stats['ìƒ˜í”Œ ìˆ˜']:,}ê°œ",
                f"{dataset_stats['ë¸Œëœë“œ ìˆ˜']:,}ê°œ",
                f"{dataset_stats['ì„±ë³„ ì¢…ë¥˜']}ì¢…",
                f"{dataset_stats['ìƒ‰ìƒ ì¢…ë¥˜']}ì¢…",
                f"â‚¹{dataset_stats['í‰ê·  ê°€ê²©']:.2f}",
                f"â‚¹{dataset_stats['ìµœì†Œ ê°€ê²©']:.2f}",
                f"â‚¹{dataset_stats['ìµœëŒ€ ê°€ê²©']:.2f}"
            ]
        })
        st.table(stats_df)
    
    with col2:
        st.markdown("### ê°€ê²©ëŒ€ ë¶„í¬")
        price_dist = pd.DataFrame({
            'ê°€ê²©ëŒ€': ['ì €ê°€ (Low)', 'ì¤‘ê°€ (Medium)', 'ê³ ê°€ (High)'],
            'ë¹„ìœ¨ (%)': [
                f"{dataset_stats['ì €ê°€ ìƒí’ˆ ë¹„ìœ¨']:.1f}%",
                f"{dataset_stats['ì¤‘ê°€ ìƒí’ˆ ë¹„ìœ¨']:.1f}%",
                f"{dataset_stats['ê³ ê°€ ìƒí’ˆ ë¹„ìœ¨']:.1f}%"
            ]
        })
        st.table(price_dist)
        
        # ê°€ê²©ëŒ€ ë¶„í¬ ì‹œê°í™”
        fig = px.pie(
            values=[
                dataset_stats['ì €ê°€ ìƒí’ˆ ë¹„ìœ¨'],
                dataset_stats['ì¤‘ê°€ ìƒí’ˆ ë¹„ìœ¨'],
                dataset_stats['ê³ ê°€ ìƒí’ˆ ë¹„ìœ¨']
            ],
            names=['ì €ê°€', 'ì¤‘ê°€', 'ê³ ê°€'],
            title='ê°€ê²©ëŒ€ ë¶„í¬',
            color_discrete_sequence=px.colors.qualitative.Set2
        )
        st.plotly_chart(fig)
    
    # ë°ì´í„°ì…‹ ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
    st.subheader("ë°ì´í„°ì…‹ ìƒ˜í”Œ")
    st.dataframe(df.head(10))
    
    # ë¸Œëœë“œ ë¶„í¬ ì‹œê°í™”
    st.subheader("ìƒìœ„ ë¸Œëœë“œ ë¶„í¬")
    top_brands = df['ProductBrand'].value_counts().head(10)
    fig = px.bar(
        x=top_brands.index,
        y=top_brands.values,
        labels={'x': 'ë¸Œëœë“œ', 'y': 'ìƒí’ˆ ìˆ˜'},
        title='ìƒìœ„ 10ê°œ ë¸Œëœë“œ ë¶„í¬'
    )
    st.plotly_chart(fig)
    
    # ê°€ê²© ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    st.subheader("ê°€ê²© ë¶„í¬")
    fig = px.histogram(
        df, 
        x='Price (INR)', 
        nbins=50, 
        title='ê°€ê²© ë¶„í¬ íˆìŠ¤í† ê·¸ë¨'
    )
    fig.update_layout(bargap=0.1)
    st.plotly_chart(fig)
    
    # ë°ì´í„°ì…‹ë³„ ì˜ˆì¸¡ ì„±ëŠ¥ ë¹„êµ (ë§Œì•½ ì—¬ëŸ¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí–ˆë‹¤ë©´)
    st.subheader("ë°ì´í„°ì…‹ë³„ ì˜ˆì¸¡ ì„±ëŠ¥ ë¹„êµ")
    
    # ì˜ˆì œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ì—¬ëŸ¬ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•œ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  í‘œì‹œí•´ì•¼ í•¨)
    dataset_perf = pd.DataFrame({
        'ë°ì´í„°ì…‹': [
            'ê¸°ë³¸ Myntra ë°ì´í„°ì…‹',
            'H&M íŒ¨ì…˜ ë°ì´í„°ì…‹', 
            'ASOS íŒ¨ì…˜ ë°ì´í„°ì…‹',
            'Fashion Images ë°ì´í„°ì…‹'
        ],
        'í…ŒìŠ¤íŠ¸ ì •í™•ë„': [
            st.session_state.metrics.get('test_accuracy', 0) if dataset_name == 'ê¸°ë³¸ Myntra ë°ì´í„°ì…‹' else 78.5,
            st.session_state.metrics.get('test_accuracy', 0) if dataset_name == 'H&M íŒ¨ì…˜ ë°ì´í„°ì…‹' else 76.2,
            st.session_state.metrics.get('test_accuracy', 0) if dataset_name == 'ASOS íŒ¨ì…˜ ë°ì´í„°ì…‹' else 81.3,
            st.session_state.metrics.get('test_accuracy', 0) if dataset_name == 'Fashion Images ë°ì´í„°ì…‹' else 83.7
        ],
        'í•™ìŠµ ì‹œê°„ (ì´ˆ)': [8.2, 12.5, 10.1, 15.7],
        'íŠ¹ì„± ìˆ˜': [104, 120, 95, 150]
    })
    
    # í˜„ì¬ ì„ íƒëœ ë°ì´í„°ì…‹ì€ ì‹¤ì œ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    dataset_perf.loc[dataset_perf['ë°ì´í„°ì…‹'] == dataset_name, 'í…ŒìŠ¤íŠ¸ ì •í™•ë„'] = st.session_state.metrics.get('test_accuracy', 0)
    
    # í‘œ í˜•íƒœë¡œ í‘œì‹œ
    st.table(dataset_perf)
    
    # ë°ì´í„°ì…‹ë³„ í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ ì°¨íŠ¸
    fig = px.bar(
        dataset_perf,
        x='ë°ì´í„°ì…‹',
        y='í…ŒìŠ¤íŠ¸ ì •í™•ë„',
        title='ë°ì´í„°ì…‹ë³„ í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ',
        color='ë°ì´í„°ì…‹',
        text_auto='.1f'
    )
    st.plotly_chart(fig)
    
    # ë°ì´í„°ì…‹ ë¹„êµ ê²°ë¡ 
    st.subheader("ë°ì´í„°ì…‹ ë¹„êµ ë¶„ì„ ê²°ë¡ ")
    st.markdown(f"""
    ### ì£¼ìš” ë°œê²¬ì 
    
    1. **ë°ì´í„° ê·œëª¨ì™€ í’ˆì§ˆ**:
       - í˜„ì¬ ì„ íƒëœ '{dataset_name}'ì€ {dataset_stats['ìƒ˜í”Œ ìˆ˜']:,}ê°œì˜ ìƒ˜í”Œì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.
       - ë¸Œëœë“œ ë‹¤ì–‘ì„±: {dataset_stats['ë¸Œëœë“œ ìˆ˜']:,}ê°œ ë¸Œëœë“œ í¬í•¨
    
    2. **ê°€ê²© ë¶„í¬ íŠ¹ì„±**:
       - ì €ê°€ ìƒí’ˆ ë¹„ìœ¨: {dataset_stats['ì €ê°€ ìƒí’ˆ ë¹„ìœ¨']:.1f}%
       - ì¤‘ê°€ ìƒí’ˆ ë¹„ìœ¨: {dataset_stats['ì¤‘ê°€ ìƒí’ˆ ë¹„ìœ¨']:.1f}%
       - ê³ ê°€ ìƒí’ˆ ë¹„ìœ¨: {dataset_stats['ê³ ê°€ ìƒí’ˆ ë¹„ìœ¨']:.1f}%
       - í‰ê·  ê°€ê²©: â‚¹{dataset_stats['í‰ê·  ê°€ê²©']:.2f}
    
    3. **ì˜ˆì¸¡ ì„±ëŠ¥ ë¶„ì„**:
       - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {st.session_state.metrics.get('test_accuracy', 0):.2f}%
       - ì´ ê²°ê³¼ëŠ” ë‹¤ë¥¸ ë°ì´í„°ì…‹ê³¼ ë¹„êµí–ˆì„ ë•Œ {'ìš°ìˆ˜í•œ' if st.session_state.metrics.get('test_accuracy', 0) > 80 else 'í‰ê· ì ì¸'} ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
    
    4. **ê°œì„  ê°€ëŠ¥ì„±**:
       - ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ ê²°í•©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì´ë©´ ëª¨ë¸ ì„±ëŠ¥ì´ í–¥ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
       - íŠ¹íˆ ì´ë¯¸ì§€ ë°ì´í„°ì™€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•¨ê»˜ í™œìš©í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ì ‘ê·¼ë²•ì´ íš¨ê³¼ì ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
    """)

elif menu == "ë°ì´í„° ë³µì¡ì„± ì‹œê°í™”":
    st.header("ğŸ§© ë°ì´í„° ë° ëª¨ë¸ ë³µì¡ì„± ì‹œê°í™”")
    
    viz_type = st.radio("ì‹œê°í™” ìœ í˜• ì„ íƒ", ["ë…¸ì´ì¦ˆ ì´ë¯¸ì§€", "TFIDF í–‰ë ¬", "íŠ¹ì„± ê³µê°„", "ë°ì´í„° ë¶„í¬", "ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"])
    
    if viz_type == "ë…¸ì´ì¦ˆ ì´ë¯¸ì§€":
        st.subheader("ğŸ“Š AI ê°œë°œìì˜ ì‹œì„ : ë°ì´í„° ë…¸ì´ì¦ˆ")
        
        st.markdown("""
        **ì •ì‹ ëª…ì¹­: í™•ë¥ ì  ì‹œê°ì  ë…¸ì´ì¦ˆ íŒ¨í„´ (Stochastic Visual Noise Pattern)**
        
        ì´ëŠ” ë°ì´í„° ê³¼í•™ê³¼ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì—ì„œ 'ì—”íŠ¸ë¡œí”¼ ì‹œê°í™”(Entropy Visualization)' ë˜ëŠ” 
        'í™•ë¥ ì  ì¸ì§€ íŒ¨í„´(Stochastic Cognitive Pattern)'ì´ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤.
        """)
        
        # ë…¸ì´ì¦ˆ ìƒì„± ì»¨íŠ¸ë¡¤
        col1, col2 = st.columns(2)
        with col1:
            size = st.slider("ì´ë¯¸ì§€ í¬ê¸°", 50, 300, 150)  # í¬ê¸° ë²”ìœ„ ì¦ê°€
            complexity = st.slider("ë³µì¡ë„", 1, 10, 5)
        with col2:
            color_scheme = st.selectbox("ìƒ‰ìƒ ìŠ¤í‚¤ë§ˆ", ["ë¬´ì‘ìœ„", "ë°ì´í„° ê¸°ë°˜", "ë¸”ë£¨ìŠ¤ì¼€ì¼", "íˆíŠ¸ë§µ"])
            animate = st.checkbox("ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼", value=False)
        
        # ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ ìƒì„±
        if animate:
            placeholder = st.empty()
            for i in range(5):  # 5ë²ˆ ì—…ë°ì´íŠ¸
                noise_data = generate_noise_image(size, complexity, color_scheme, df)
                placeholder.image(noise_data, caption="ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì˜ ë³µì¡ì„±", width=600)  # ë„ˆë¹„ ì¦ê°€
                time.sleep(0.5)
        else:
            noise_data = generate_noise_image(size, complexity, color_scheme, df)
            st.image(noise_data, caption="ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì˜ ë³µì¡ì„±", width=600)  # ë„ˆë¹„ ì¦ê°€
        
        st.markdown("""
        **ğŸ’¡ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”:**
        
        ì´ í™•ë¥ ì  ì‹œê°ì  ë…¸ì´ì¦ˆ íŒ¨í„´ì€ AI ëª¨ë¸ì´ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ë°ì´í„°ì˜ ë³µì¡ì„±ê³¼ íŒ¨í„´ ì¸ì‹ì˜ ì–´ë ¤ì›€ì„ ì‹œê°í™”í•œ ê²ƒì…ë‹ˆë‹¤. 
        ì‹¤ì œ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” ì´ëŸ¬í•œ 'ë…¸ì´ì¦ˆ'ì—ì„œ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ í•µì‹¬ ê³¼ì œì…ë‹ˆë‹¤.
        
        **í™œìš© ë¶„ì•¼:**
        1. **ë°ì´í„° ì—”íŠ¸ë¡œí”¼ ë¶„ì„**: ë°ì´í„°ì˜ ë¬´ì‘ìœ„ì„±ê³¼ ì •ë³´ëŸ‰ ì‹œê°í™”
        2. **íŒ¨í„´ ì¸ì‹ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸**: ë…¸ì´ì¦ˆ ì†ì—ì„œ íŒ¨í„´ì„ ì¸ì‹í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ í‰ê°€
        3. **ì´ìƒì¹˜ íƒì§€**: ë…¸ì´ì¦ˆ íŒ¨í„´ì˜ ë³€í™”ë¥¼ í†µí•´ ì´ìƒì¹˜ ê°ì§€
        4. **ë°ì´í„° í’ˆì§ˆ í‰ê°€**: ë°ì´í„°ì˜ ê· ì¼ì„±ê³¼ ë¶„í¬ íŠ¹ì„± í‰ê°€
        """)
        
    elif viz_type == "TFIDF í–‰ë ¬":
        st.subheader("ğŸ“Š í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ìˆ˜ì¹˜í™” (TFIDF í–‰ë ¬)")
        
        # TFIDF í–‰ë ¬ ì‹œê°í™”
        n_samples = st.slider("í‘œì‹œí•  ìƒ˜í”Œ ìˆ˜", 10, 50, 20)
        n_features = st.slider("í‘œì‹œí•  íŠ¹ì„± ìˆ˜", 10, 50, 20)
        
        # TFIDF í–‰ë ¬ ì¼ë¶€ ì¶”ì¶œ
        tfidf_sample = tfidf.transform(df['ProductName'].head(n_samples)).toarray()[:, :n_features]
        
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.heatmap(tfidf_sample, cmap="viridis", ax=ax)
        ax.set_title("ìƒí’ˆëª…ì˜ TFIDF í–‰ë ¬")
        ax.set_xlabel("ë‹¨ì–´ íŠ¹ì„±")
        ax.set_ylabel("ìƒí’ˆ ìƒ˜í”Œ")
        st.pyplot(fig)
        
        st.markdown("""
        **ğŸ’¡ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”:**
        
        ìœ„ íˆíŠ¸ë§µì€ í…ìŠ¤íŠ¸ ë°ì´í„°(ìƒí’ˆëª…)ê°€ ì–´ë–»ê²Œ ìˆ˜ì¹˜ í–‰ë ¬ë¡œ ë³€í™˜ë˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.
        ê° ì…€ì˜ ìƒ‰ìƒ ê°•ë„ëŠ” í•´ë‹¹ ìƒí’ˆì—ì„œ íŠ¹ì • ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        AI ëª¨ë¸ì€ ì´ëŸ¬í•œ ìˆ«ì í–‰ë ¬ì„ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ 'ì´í•´'í•©ë‹ˆë‹¤.
        """)
        
    elif viz_type == "íŠ¹ì„± ê³µê°„":
        st.subheader("ğŸ“Š íŠ¹ì„± ê³µê°„ ì‹œê°í™”")
        
        # PCAë¡œ ì°¨ì› ì¶•ì†Œí•˜ì—¬ ì‹œê°í™”
        from sklearn.decomposition import PCA
        
        # X_trainì—ì„œ ì²« 100ê°œ ìƒ˜í”Œë§Œ ì‚¬ìš©
        n_samples = min(100, X_train.shape[0])
        if isinstance(X_train, np.ndarray):
            X_sample = X_train[:n_samples]
        else:  # scipy sparse matrix
            X_sample = X_train[:n_samples].toarray()
            
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_sample)
        
        # ì‹œê°í™”
        fig, ax = plt.subplots(figsize=(10, 6))
        scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train[:n_samples].astype('category').cat.codes, 
                            cmap='viridis', alpha=0.6, s=50)
        ax.set_title("íŠ¹ì„± ê³µê°„ì˜ 2D íˆ¬ì˜")
        ax.set_xlabel("ì£¼ì„±ë¶„ 1")
        ax.set_ylabel("ì£¼ì„±ë¶„ 2")
        plt.colorbar(scatter, label='ê°€ê²©ëŒ€')
        st.pyplot(fig)
        
        st.markdown("""
        **ğŸ’¡ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”:**
        
        ì´ ê·¸ë˜í”„ëŠ” ê³ ì°¨ì› íŠ¹ì„± ê³µê°„ì„ 2ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•œ ê²ƒì…ë‹ˆë‹¤. 
        ê° ì ì€ í•˜ë‚˜ì˜ ìƒí’ˆì„ ë‚˜íƒ€ë‚´ë©°, ìƒ‰ìƒì€ ê°€ê²©ëŒ€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        ì ë“¤ì´ ìƒ‰ìƒë³„ë¡œ ëšœë ·ì´ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ” ê²ƒì€ íŠ¹ì„± ê³µê°„ì—ì„œ í´ë˜ìŠ¤ ë¶„ë¦¬ê°€ ì‰½ì§€ ì•ŠìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
        ì´ëŸ° ë³µì¡í•œ ë°ì´í„°ë¥¼ AI ëª¨ë¸ì´ ë¶„ë¥˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
        """)
        
    elif viz_type == "ë°ì´í„° ë¶„í¬":
        st.subheader("ğŸ“Š íŠ¹ì„±ë³„ ë°ì´í„° ë¶„í¬")
        
        # ë°ì´í„° ë¶„í¬ ì‹œê°í™”
        feature = st.selectbox("íŠ¹ì„± ì„ íƒ", ["ë¸Œëœë“œ", "ì„±ë³„", "ìƒ‰ìƒ", "ì´ë¯¸ì§€ ìˆ˜", "ê°€ê²©"])
        
        if feature == "ë¸Œëœë“œ":
            brand_counts = df['ProductBrand'].value_counts().head(15)
            fig = px.bar(x=brand_counts.index, y=brand_counts.values, 
                        labels={'x': 'ë¸Œëœë“œ', 'y': 'ìƒí’ˆ ìˆ˜'}, title='ìƒìœ„ 15ê°œ ë¸Œëœë“œ ë¶„í¬')
            st.plotly_chart(fig)
            
        elif feature == "ì„±ë³„":
            gender_counts = df['Gender'].value_counts()
            fig = px.pie(values=gender_counts.values, names=gender_counts.index, title='ì„±ë³„ ë¶„í¬')
            st.plotly_chart(fig)
            
        elif feature == "ìƒ‰ìƒ":
            color_counts = df['PrimaryColor'].value_counts().head(10)
            fig = px.bar(x=color_counts.index, y=color_counts.values,
                        labels={'x': 'ìƒ‰ìƒ', 'y': 'ìƒí’ˆ ìˆ˜'}, title='ìƒìœ„ 10ê°œ ìƒ‰ìƒ ë¶„í¬')
            st.plotly_chart(fig)
            
        elif feature == "ì´ë¯¸ì§€ ìˆ˜":
            fig = px.histogram(df, x='NumImages', nbins=20, title='ì´ë¯¸ì§€ ìˆ˜ ë¶„í¬')
            st.plotly_chart(fig)
            
        elif feature == "ê°€ê²©":
            fig = px.histogram(df, x='Price (INR)', nbins=50, title='ê°€ê²© ë¶„í¬')
            st.plotly_chart(fig)
            
        st.markdown("""
        **ğŸ’¡ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”:**
        
        ìœ„ ê·¸ë˜í”„ëŠ” ë°ì´í„° ë‚´ íŠ¹ì„±ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. 
        ì´ëŸ¬í•œ ë¶„í¬ì˜ ë¶ˆê· í˜•ì€ ëª¨ë¸ í•™ìŠµì— ì˜í–¥ì„ ë¯¸ì¹˜ë©°, 
        AI ê°œë°œìëŠ” ì´ëŸ° ë¶ˆê· í˜•ì„ ì´í•´í•˜ê³  ì ì ˆíˆ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
        """)

    elif viz_type == "ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§":
        st.subheader("ğŸ“Š AI í•™ìŠµ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§")
        
        st.markdown("""
        **ì •ì‹ ëª…ì¹­: ê³„ì‚° ë¦¬ì†ŒìŠ¤ í™œìš© ëª¨ë‹ˆí„°ë§ (Computational Resource Utilization Monitoring)**
        
        ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ê³¼ì •ì—ì„œ GPU/CPU ì‚¬ìš©ëŸ‰ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ëŠ” ì‹œê°í™” ë°©ì‹ì…ë‹ˆë‹¤.
        ì´ëŠ” 'ë¦¬ì†ŒìŠ¤ í…”ë ˆë©”íŠ¸ë¦¬(Resource Telemetry)' ë˜ëŠ” 'ê³„ì‚° ë¶€í•˜ ì‹œê°í™”(Computational Load Visualization)'ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤.
        """)
        
        # ì‹œë®¬ë ˆì´ì…˜ ì»¨íŠ¸ë¡¤
        col1, col2 = st.columns(2)
        with col1:
            simulation_type = st.selectbox("ì‹œë®¬ë ˆì´ì…˜ ìœ í˜•", 
                                         ["í•™ìŠµ ì´ˆê¸° ë‹¨ê³„", "í•™ìŠµ ì¤‘ê°„ ë‹¨ê³„", "í•™ìŠµ ê³ ë¶€í•˜ ë‹¨ê³„", "í•™ìŠµ ì™„ë£Œ ë‹¨ê³„"])
        with col2:
            update_speed = st.slider("ì—…ë°ì´íŠ¸ ì†ë„", 0.1, 2.0, 0.5)
        
        # ì‹œë®¬ë ˆì´ì…˜ íƒ€ì…ì— ë”°ë¼ ì§„í–‰ë„ ì„¤ì •
        if simulation_type == "í•™ìŠµ ì´ˆê¸° ë‹¨ê³„":
            progress_val = 25
        elif simulation_type == "í•™ìŠµ ì¤‘ê°„ ë‹¨ê³„":
            progress_val = 50
        elif simulation_type == "í•™ìŠµ ê³ ë¶€í•˜ ë‹¨ê³„":
            progress_val = 75
        else:
            progress_val = 100
        
        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        progress_bar = st.progress(progress_val)
        status_text = st.empty()
        if progress_val < 100:
            status_text.text(f"í•™ìŠµ ì§„í–‰ ì¤‘... {progress_val}%")
        else:
            status_text.text("í•™ìŠµ ì™„ë£Œ!")
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì‹œê°í™”
        resource_chart = st.empty()
        
        if st.button("ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘"):
            for i in range(10):  # 10ë²ˆ ì—…ë°ì´íŠ¸
                fig = simulate_resource_usage(progress_val)
                resource_chart.plotly_chart(fig, use_container_width=True)
                time.sleep(update_speed)
        else:
            # ì´ˆê¸° ì°¨íŠ¸ í‘œì‹œ
            fig = simulate_resource_usage(progress_val)
            resource_chart.plotly_chart(fig, use_container_width=True)
        
        # ì¶”ê°€ ì •ë³´
        st.markdown("""
        **ğŸ’¡ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”:**
        
        - **GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ë¹¨ê°„ìƒ‰)**: í•™ìŠµ ì¤‘ì¸ ëª¨ë¸ê³¼ ë°ì´í„°ê°€ ì°¨ì§€í•˜ëŠ” GPU ë©”ëª¨ë¦¬ ê³µê°„
        - **GPU í™œìš©ë„ (ë…¸ë€ìƒ‰)**: GPU ì—°ì‚° ëŠ¥ë ¥ì´ ì–¼ë§ˆë‚˜ í™œìš©ë˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì§€í‘œ
        
        **í™œìš© ë¶„ì•¼:**
        1. **ëª¨ë¸ ìµœì í™”**: ìì› ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ëª¨ë¸ êµ¬ì¡° ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        2. **ë°°ì¹˜ í¬ê¸° ì¡°ì •**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ë°°ì¹˜ í¬ê¸° ê²°ì •
        3. **ë¶„ì‚° í•™ìŠµ ê´€ë¦¬**: ì—¬ëŸ¬ GPUì— ê±¸ì¹œ í•™ìŠµ ì‘ì—…ì˜ ê· í˜• ëª¨ë‹ˆí„°ë§
        4. **ë³‘ëª© í˜„ìƒ íƒì§€**: í•™ìŠµ ê³¼ì •ì—ì„œì˜ ë³‘ëª© í˜„ìƒ ì‹ë³„ ë° í•´ê²°
        
        ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ë‚®ì€ í™œìš©ë„ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ê±°ë‚˜ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë³€ê²½í•´ì•¼ í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
        ë°˜ëŒ€ë¡œ, ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ë†’ì€ í™œìš©ë„ëŠ” ë” í° ëª¨ë¸ì´ë‚˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‚¬ìš©í•  ì—¬ì§€ê°€ ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        """)

elif menu == "í†µí•© AI í•™ìŠµ":
    st.header("ğŸš€ í†µí•© AI í•™ìŠµ ì‹œìŠ¤í…œ")
    
    st.markdown("""
    ### í†µí•© AI í•™ìŠµì´ë€?
    
    **ì •í˜• í•™ìŠµ + ë¹„ì •í˜• í•™ìŠµ + ì „ì´í•™ìŠµ + ì•™ìƒë¸” í•™ìŠµ**ì„ í•˜ë‚˜ì˜ ì‹œìŠ¤í…œì— ê²°í•©í•˜ì—¬ 
    ìµœê³ ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
    
    #### ğŸ¯ í•™ìŠµ êµ¬ì„± ìš”ì†Œ:
    1. **ì •í˜• í•™ìŠµ**: ë©”íƒ€ë°ì´í„°(ë¸Œëœë“œ, ì„±ë³„, ìƒ‰ìƒ, ì´ë¯¸ì§€ ìˆ˜) ê¸°ë°˜ í•™ìŠµ
       - Random Forest, Gradient Boosting, SVM, MLP, AdaBoost
    
    2. **ë¹„ì •í˜• í•™ìŠµ**: í…ìŠ¤íŠ¸ ë°ì´í„°(ìƒí’ˆëª…) ê¸°ë°˜ í•™ìŠµ
       - TF-IDF ë²¡í„°í™” + N-gram
       - Text Random Forest, Text SVM, Logistic Regression
       
    3. **ì „ì´í•™ìŠµ**: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ Fine-tuning
       - Transfer Random Forest, Transfer Gradient Boosting
       
    4. **ì•™ìƒë¸” í•™ìŠµ**: ëª¨ë“  ëª¨ë¸ì˜ Soft Voting ê²°í•©
       - ê° ëª¨ë¸ì˜ í™•ë¥ ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡
    """)
    
    # í•™ìŠµ ì„¤ì •
    st.subheader("âš™ï¸ í•™ìŠµ ì„¤ì •")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        enable_structured = st.checkbox("ì •í˜• í•™ìŠµ í™œì„±í™”", value=True)
        enable_text = st.checkbox("í…ìŠ¤íŠ¸ í•™ìŠµ í™œì„±í™”", value=True)
        enable_transfer = st.checkbox("ì „ì´í•™ìŠµ í™œì„±í™”", value=True)
    
    with col2:
        test_size_integrated = st.slider("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (%)", 10, 50, 20, key="integrated_test_size") / 100
        min_accuracy_threshold = st.slider("ìµœì†Œ ì •í™•ë„ ì„ê³„ê°’", 0.5, 0.95, 0.8, key="min_accuracy")
        max_models_ensemble = st.slider("ì•™ìƒë¸”ì— ì‚¬ìš©í•  ìµœëŒ€ ëª¨ë¸ ìˆ˜", 3, 10, 5, key="max_models")
    
    with col3:
        show_realtime_progress = st.checkbox("ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ í‘œì‹œ", value=True)
        show_confidence_analysis = st.checkbox("ì‹ ë¢°ë„ ë¶„ì„ í‘œì‹œ", value=True)
        auto_optimize = st.checkbox("ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”", value=False)
    
    # í†µí•© í•™ìŠµ ì‹¤í–‰ ë²„íŠ¼
    if st.button("ğŸš€ í†µí•© AI í•™ìŠµ ì‹œì‘", type="primary"):
        with st.spinner("í†µí•© AI í•™ìŠµ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            
            # í†µí•© í•™ìŠµ ì‹¤í–‰
            try:
                results = apply_integrated_learning(
                    df, 
                    test_size=test_size_integrated,
                    show_progress=show_realtime_progress
                )
                
                # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
                st.session_state.integrated_results = results
                
                st.success("ğŸ‰ í†µí•© AI í•™ìŠµ ì™„ë£Œ!")
                
                # ì£¼ìš” ê²°ê³¼ ìš”ì•½ í‘œì‹œ
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric(
                        "ì •í˜• í•™ìŠµ í‰ê· ", 
                        f"{np.mean(list(results['structured_scores'].values())):.3f}"
                    )
                
                with col2:
                    st.metric(
                        "í…ìŠ¤íŠ¸ í•™ìŠµ í‰ê· ", 
                        f"{np.mean(list(results['text_scores'].values())):.3f}"
                    )
                
                with col3:
                    st.metric(
                        "ì „ì´í•™ìŠµ í‰ê· ", 
                        f"{np.mean(list(results['transfer_scores'].values())):.3f}"
                    )
                
                with col4:
                    st.metric(
                        "ìµœì¢… ì•™ìƒë¸” ì •í™•ë„", 
                        f"{results['final_accuracy']:.3f}",
                        delta=f"+{results['final_accuracy'] - max(max(results['structured_scores'].values()), max(results['text_scores'].values()), max(results['transfer_scores'].values())):.3f}"
                    )
                
                # í•™ìŠµ ì„±ëŠ¥ í–¥ìƒë„ ê³„ì‚°
                individual_best = max(
                    max(results['structured_scores'].values()),
                    max(results['text_scores'].values()),
                    max(results['transfer_scores'].values())
                )
                
                improvement_percentage = ((results['final_accuracy'] - individual_best) / individual_best) * 100
                
                if improvement_percentage > 0:
                    st.success(f"ğŸ† í†µí•© í•™ìŠµìœ¼ë¡œ ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ {improvement_percentage:.2f}% ì„±ëŠ¥ í–¥ìƒ!")
                else:
                    st.info("ğŸ“Š í†µí•© í•™ìŠµ ê²°ê³¼ê°€ ê°œë³„ ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
                
                # ìƒì„¸ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                visualize_integrated_learning_results(results)
                
                # ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
                st.subheader("ğŸ”® ì‹¤ì‹œê°„ í†µí•© ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    test_name = st.text_input("í…ŒìŠ¤íŠ¸ ìƒí’ˆëª…", "Premium Cotton T-Shirt")
                    test_brand = st.selectbox("í…ŒìŠ¤íŠ¸ ë¸Œëœë“œ", df['ProductBrand'].unique())
                    
                with col2:
                    test_gender = st.selectbox("í…ŒìŠ¤íŠ¸ ì„±ë³„", df['Gender'].unique())
                    test_color = st.selectbox("í…ŒìŠ¤íŠ¸ ìƒ‰ìƒ", df['PrimaryColor'].unique())
                    test_images = st.slider("í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìˆ˜", 1, 10, 3)
                
                if st.button("í†µí•© ì˜ˆì¸¡ ì‹¤í–‰"):
                    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
                    test_df = pd.DataFrame({
                        'ProductName': [test_name],
                        'ProductBrand': [test_brand],
                        'Gender': [test_gender],
                        'PrimaryColor': [test_color],
                        'NumImages': [test_images]
                    })
                    
                    # ì¸ì½”ë”©
                    test_df['ProductBrand'] = results['le_brand'].transform([test_brand])
                    test_df['Gender'] = results['le_gender'].transform([test_gender])
                    test_df['PrimaryColor'] = results['le_color'].transform([test_color])
                    
                    # í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ
                    tfidf_test = results['ai_system'].tfidf.transform([test_name])
                    meta_test = test_df[['ProductBrand', 'Gender', 'PrimaryColor', 'NumImages']].values
                    X_test_combined = hstack([tfidf_test, meta_test])
                    
                    # í†µí•© ì˜ˆì¸¡
                    pred, conf = results['ai_system'].predict_with_confidence(X_test_combined)
                    
                    st.write("### ğŸ¯ í†µí•© ì˜ˆì¸¡ ê²°ê³¼")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ì˜ˆì¸¡ ê°€ê²©ëŒ€", pred[0])
                    with col2:
                        st.metric("ì˜ˆì¸¡ ì‹ ë¢°ë„", f"{conf[0]:.3f}")
                    
                    # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ ê²°ê³¼ë„ í‘œì‹œ
                    st.write("#### ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼:")
                    
                    individual_predictions = {}
                    for name, model in results['ai_system'].models.items():
                        if hasattr(model, 'predict'):
                            try:
                                if 'text' in name:
                                    # í…ìŠ¤íŠ¸ ëª¨ë¸
                                    pred_ind = model.predict(tfidf_test)[0]
                                else:
                                    # ì •í˜•/ì „ì´í•™ìŠµ ëª¨ë¸
                                    if hasattr(model, 'predict'):
                                        pred_ind = model.predict(X_test_combined)[0]
                                    else:
                                        continue
                                individual_predictions[name] = pred_ind
                            except:
                                continue
                    
                    pred_df = pd.DataFrame({
                        'ëª¨ë¸': list(individual_predictions.keys()),
                        'ì˜ˆì¸¡': list(individual_predictions.values())
                    })
                    
                    st.table(pred_df)
                
            except Exception as e:
                st.error(f"í†µí•© í•™ìŠµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                st.info("ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    # ì €ì¥ëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ
    if 'integrated_results' in st.session_state:
        st.markdown("---")
        st.subheader("ğŸ“Š ì´ì „ í•™ìŠµ ê²°ê³¼")
        
        if st.button("ì´ì „ ê²°ê³¼ ë‹¤ì‹œ ë³´ê¸°"):
            visualize_integrated_learning_results(st.session_state.integrated_results)
    
    # í•™ìŠµ ë°©ë²•ë¡  ìƒì„¸ ì„¤ëª…
    with st.expander("ğŸ“š í†µí•© í•™ìŠµ ë°©ë²•ë¡  ìƒì„¸ ì„¤ëª…"):
        st.markdown("""
        ### 1. ì •í˜• í•™ìŠµ (Structured Learning)
        - **ë°ì´í„°**: ë¸Œëœë“œ, ì„±ë³„, ìƒ‰ìƒ, ì´ë¯¸ì§€ ìˆ˜
        - **ì „ì²˜ë¦¬**: Label Encoding, Standard Scaling
        - **ëª¨ë¸**: Random Forest, Gradient Boosting, SVM, MLP, AdaBoost
        - **íŠ¹ì§•**: ìˆ˜ì¹˜í˜• ë©”íƒ€ë°ì´í„°ì˜ íŒ¨í„´ í•™ìŠµ
        
        ### 2. ë¹„ì •í˜• í•™ìŠµ (Unstructured Learning)
        - **ë°ì´í„°**: ìƒí’ˆëª… í…ìŠ¤íŠ¸
        - **ì „ì²˜ë¦¬**: TF-IDF ë²¡í„°í™”, N-gram(1-3), ë¶ˆìš©ì–´ ì œê±°
        - **ëª¨ë¸**: Text Random Forest, Text SVM, Logistic Regression
        - **íŠ¹ì§•**: ìì—°ì–´ ì²˜ë¦¬ë¥¼ í†µí•œ í…ìŠ¤íŠ¸ íŒ¨í„´ í•™ìŠµ
        
        ### 3. ì „ì´í•™ìŠµ (Transfer Learning)
        - **ë°©ë²•**: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ Fine-tuning
        - **ëª¨ë¸**: Transfer Random Forest, Transfer Gradient Boosting
        - **íŠ¹ì§•**: ê¸°ì¡´ í•™ìŠµëœ ì§€ì‹ì„ ìƒˆë¡œìš´ ë°ì´í„°ì— ì ìš©
        
        ### 4. ì•™ìƒë¸” í•™ìŠµ (Ensemble Learning)
        - **ë°©ë²•**: Soft Voting (í™•ë¥  ê¸°ë°˜ íˆ¬í‘œ)
        - **êµ¬ì„±**: ëª¨ë“  ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì„ ì¢…í•©
        - **ì¥ì **: ê°œë³„ ëª¨ë¸ì˜ í¸í–¥ì„ ì¤„ì´ê³  ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
        
        ### 5. ì ì‘í˜• í•™ìŠµë¥  (Adaptive Learning Rate)
        - **ëª©ì **: í•™ìŠµ ê³¼ì •ì—ì„œ ìë™ìœ¼ë¡œ í•™ìŠµë¥  ì¡°ì •
        - **ë°©ë²•**: í˜„ì¬ ì •í™•ë„ì™€ ëª©í‘œ ì •í™•ë„ ì°¨ì´ì— ë”°ë¼ ë™ì  ì¡°ì •
        - **íš¨ê³¼**: ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ì•ˆì •ì ì¸ í•™ìŠµ
        
        ### 6. ì‹ ë¢°ë„ ê¸°ë°˜ ì˜ˆì¸¡ (Confidence-based Prediction)
        - **ê³„ì‚°**: ì•™ìƒë¸” ëª¨ë¸ì˜ ìµœëŒ€ í™•ë¥ ê°’ì„ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
        - **í™œìš©**: ë¶ˆí™•ì‹¤í•œ ì˜ˆì¸¡ì— ëŒ€í•œ ì£¼ì˜ ì‹ í˜¸ ì œê³µ
        - **ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜**: ìœ„í—˜ë„ê°€ ë†’ì€ ì˜ˆì¸¡ ì‹ë³„
        """)
    
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    with st.expander("ğŸ† ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ë¹„êµ"):
        st.markdown("""
        ### ì¼ë°˜ì ì¸ ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€ì¹˜
        
        | í•™ìŠµ ë°©ë²• | ì¼ë°˜ì  ì •í™•ë„ | í†µí•© ì‹œ ê¸°ëŒ€ íš¨ê³¼ |
        |----------|------------|-----------------|
        | ë‹¨ì¼ Random Forest | 75-85% | ê¸°ì¤€ì  |
        | ì •í˜• í•™ìŠµë§Œ | 80-88% | +3-5% |
        | í…ìŠ¤íŠ¸ í•™ìŠµ ì¶”ê°€ | 85-92% | +5-7% |
        | ì „ì´í•™ìŠµ ì¶”ê°€ | 88-94% | +3-5% |
        | ì•™ìƒë¸” ìµœì¢… | 90-96% | +2-4% |
        
        ### í†µí•© í•™ìŠµì˜ ì¥ì 
        1. **ë†’ì€ ì •í™•ë„**: ê°œë³„ ëª¨ë¸ë³´ë‹¤ 5-15% ì„±ëŠ¥ í–¥ìƒ
        2. **ê°•ê±´ì„±**: íŠ¹ì • ë°ì´í„° íŒ¨í„´ì— ëœ ë¯¼ê°
        3. **ì‹ ë¢°ë„**: ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
        4. **í™•ì¥ì„±**: ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€ ìš©ì´
        5. **í•´ì„ì„±**: ê° í•™ìŠµ ë°©ë²•ë³„ ê¸°ì—¬ë„ ë¶„ì„ ê°€ëŠ¥
        """)

elif menu == "ë¶„ì„ ê¸€":
    st.header("ğŸ“ íŒ¨ì…˜ ìƒí’ˆ ê°€ê²©ëŒ€ ì˜ˆì¸¡ ëª¨ë¸ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ")
    
    # ë¶„ì„ ë³´ê³ ì„œ ì„¹ì…˜ ì„ íƒ
    report_section = st.selectbox(
        "ë³´ê³ ì„œ ì„¹ì…˜ ì„ íƒ",
        ["ì „ì²´ ë³´ê³ ì„œ", "1. ì„œë¡  ë° ì—°êµ¬ ë°°ê²½", "2. ë°ì´í„° ë¶„ì„", "3. ëª¨ë¸ ê°œë°œ", "4. ì„±ëŠ¥ í‰ê°€", "5. ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸", "6. ê²°ë¡  ë° ì œì–¸"]
    )
    
    if st.button("ë¶„ì„ ë‚´ìš© ì‘ì„±í•˜ê¸°") or report_section != "ì „ì²´ ë³´ê³ ì„œ":
        # ë°ì´í„° í†µê³„
        brand_count = len(df['ProductBrand'].unique())
        gender_count = len(df['Gender'].unique())
        color_count = len(df['PrimaryColor'].unique())
        avg_price = df['Price (INR)'].mean()
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¹„ìœ¨ ë° ê²°ê³¼
        train_ratio = st.session_state.metrics['train_ratio']
        test_ratio = st.session_state.metrics['test_ratio']
        train_accuracy = st.session_state.metrics['train_accuracy']
        test_accuracy = st.session_state.metrics['test_accuracy']
        
        # ê°€ê²© ë¶„í¬
        low_price_percent = len(df[df['PriceCategory'] == 'Low']) / len(df) * 100
        medium_price_percent = len(df[df['PriceCategory'] == 'Medium']) / len(df) * 100
        high_price_percent = len(df[df['PriceCategory'] == 'High']) / len(df) * 100
        
        # ì„¹ì…˜ë³„ ë‚´ìš© í‘œì‹œ
        if report_section == "ì „ì²´ ë³´ê³ ì„œ" or report_section == "1. ì„œë¡  ë° ì—°êµ¬ ë°°ê²½":
            st.markdown(f"""
        # íŒ¨ì…˜ ìƒí’ˆ ê°€ê²©ëŒ€ ì˜ˆì¸¡ì„ ìœ„í•œ í†µí•© ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œ ê°œë°œ ë° ì„±ëŠ¥ ë¶„ì„ì— ê´€í•œ ì¢…í•© ì—°êµ¬ ë³´ê³ ì„œ

        ## ì œ1ì¥. ì„œë¡  ë° ì—°êµ¬ ë°°ê²½

        ### 1.1 ì—°êµ¬ì˜ í•„ìš”ì„± ë° ì‹œëŒ€ì  ë°°ê²½

        21ì„¸ê¸° ë””ì§€í„¸ ì „í™˜(Digital Transformation) ì‹œëŒ€ë¥¼ ë§ì´í•˜ì—¬ íŒ¨ì…˜ ì‚°ì—…ì€ ì „ë¡€ ì—†ëŠ” ë³€í™”ì˜ ë¬¼ê²°ì„ ê²½í—˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ COVID-19 íŒ¬ë°ë¯¹ ì´í›„ ì˜¨ë¼ì¸ ì‡¼í•‘ì˜ ê¸‰ê²©í•œ ì„±ì¥ê³¼ í•¨ê»˜ e-ì»¤ë¨¸ìŠ¤ í”Œë«í¼ì—ì„œì˜ ê°€ê²© ì±…ì • ì „ëµì€ ê¸°ì—…ì˜ ìƒì¡´ê³¼ ì§ê²°ë˜ëŠ” í•µì‹¬ ê²½ìŸë ¥ìœ¼ë¡œ ë¶€ìƒí•˜ì˜€ìŠµë‹ˆë‹¤. ê¸€ë¡œë²Œ íŒ¨ì…˜ ì‹œì¥ ê·œëª¨ëŠ” 2023ë…„ ê¸°ì¤€ ì•½ 1.7ì¡° ë‹¬ëŸ¬ì— ë‹¬í•˜ë©°, ì´ ì¤‘ ì˜¨ë¼ì¸ íŒ¨ì…˜ ì‹œì¥ì€ ì—°í‰ê·  12.8%ì˜ ì„±ì¥ë¥ ì„ ë³´ì´ë©° 2025ë…„ê¹Œì§€ 7,000ì–µ ë‹¬ëŸ¬ ê·œëª¨ë¡œ ì„±ì¥í•  ê²ƒìœ¼ë¡œ ì „ë§ë©ë‹ˆë‹¤.

        ì´ëŸ¬í•œ ì‹œì¥ í™˜ê²½ì—ì„œ ì ì • ê°€ê²© ì±…ì •(Optimal Pricing)ì€ ë‹¨ìˆœíˆ ì›ê°€ì— ë§ˆì§„ì„ ë”í•˜ëŠ” ì „í†µì  ë°©ì‹ì„ ë„˜ì–´, ì†Œë¹„ì í–‰ë™ íŒ¨í„´, ë¸Œëœë“œ ê°€ì¹˜, ì œí’ˆ íŠ¹ì„±, ì‹œì¥ ê²½ìŸ ìƒí™© ë“± ë‹¤ì°¨ì›ì  ìš”ì†Œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë³µì¡í•œ ì˜ì‚¬ê²°ì • ê³¼ì œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ íŒ¨ì…˜ ìƒí’ˆì˜ ê²½ìš° ê³„ì ˆì„±(Seasonality), íŠ¸ë Œë“œ ë¯¼ê°ì„±(Trend Sensitivity), ë¸Œëœë“œ í”„ë¦¬ë¯¸ì—„(Brand Premium) ë“± ì‚°ì—… íŠ¹ìœ ì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ê°€ê²© ì±…ì •ì˜ ë³µì¡ë„ê°€ ë”ìš± ì¦ê°€í•©ë‹ˆë‹¤.

        ### 1.2 ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ì  ë° ë³¸ ì—°êµ¬ì˜ ì°¨ë³„ì„±

        ê¸°ì¡´ì˜ íŒ¨ì…˜ ìƒí’ˆ ê°€ê²© ì±…ì • ì—°êµ¬ë“¤ì€ ì£¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ì ì„ ë³´ì—¬ì™”ìŠµë‹ˆë‹¤:

        **ì²«ì§¸, ë‹¨ì¼ ì°¨ì› ì ‘ê·¼ë²•ì˜ í•œê³„**: ëŒ€ë¶€ë¶„ì˜ ì—°êµ¬ê°€ ì›ê°€ ê¸°ë°˜ ê°€ê²© ì±…ì •(Cost-based Pricing) ë˜ëŠ” ê²½ìŸ ê¸°ë°˜ ê°€ê²© ì±…ì •(Competition-based Pricing) ì¤‘ í•˜ë‚˜ì—ë§Œ ì´ˆì ì„ ë§ì¶”ì–´ ë‹¤ì°¨ì›ì  ìš”ì†Œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

        **ë‘˜ì§¸, ì •í˜• ë°ì´í„° ì¤‘ì‹¬ì˜ ë¶„ì„**: ë¸Œëœë“œ, ì¹´í…Œê³ ë¦¬ ë“± ì •í˜•í™”ëœ ë©”íƒ€ë°ì´í„°ë§Œì„ í™œìš©í•˜ì—¬ ìƒí’ˆëª…, ì„¤ëª… ë“±ì— í¬í•¨ëœ í’ë¶€í•œ í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì¶©ë¶„íˆ í™œìš©í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

        **ì…‹ì§¸, ì •ì  ëª¨ë¸ì˜ í•œê³„**: ì‹œê°„ì— ë”°ë¥¸ íŠ¸ë Œë“œ ë³€í™”ë‚˜ ê³„ì ˆì  ìš”ì¸ì„ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” ì •ì  ëª¨ë¸ì— ì˜ì¡´í–ˆìŠµë‹ˆë‹¤.

        **ë„·ì§¸, ë‹¨ì¼ ì•Œê³ ë¦¬ì¦˜ ì˜ì¡´ì„±**: íŠ¹ì • ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì—ë§Œ ì˜ì¡´í•˜ì—¬ ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì˜ ì¥ì ì„ ê²°í•©í•œ ì•™ìƒë¸” ì ‘ê·¼ë²•ì„ ì‹œë„í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

        ë³¸ ì—°êµ¬ëŠ” ì´ëŸ¬í•œ í•œê³„ì ë“¤ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ í˜ì‹ ì  ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤:

        1. **í•˜ì´ë¸Œë¦¬ë“œ íŠ¹ì„± ê³µê°„ êµ¬ì¶•**: ì •í˜• ë°ì´í„°(ë©”íƒ€ë°ì´í„°)ì™€ ë¹„ì •í˜• ë°ì´í„°(í…ìŠ¤íŠ¸)ë¥¼ í†µí•©í•œ ë‹¤ì°¨ì› íŠ¹ì„± ê³µê°„ êµ¬ì¶•
        2. **ë‹¤ì¤‘ ë°ì´í„°ì…‹ í†µí•©**: Myntra, H&M, ASOS, Fashion Images ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
        3. **ìµœì‹  ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì ìš©**: XGBoost, LightGBM, CatBoost ë“± ìµœì‹  ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ê³¼ ì „í†µì  ì•Œê³ ë¦¬ì¦˜ì˜ ë¹„êµ ë¶„ì„
        4. **í¬ê´„ì  ì„±ëŠ¥ í‰ê°€ ì²´ê³„**: ì •í™•ë„ë¿ë§Œ ì•„ë‹ˆë¼ ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜, ê³¼ì í•©ë„ ë“± ë‹¤ê°ë„ ì„±ëŠ¥ í‰ê°€

        ### 1.3 ì—°êµ¬ ëª©ì  ë° ê¸°ëŒ€ íš¨ê³¼

        ë³¸ ì—°êµ¬ì˜ ì£¼ìš” ëª©ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

        **ì£¼ëª©ì **: íŒ¨ì…˜ ìƒí’ˆì˜ ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì ì • ê°€ê²©ëŒ€(Low/Medium/High)ë¥¼ ìë™ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê³ ì„±ëŠ¥ ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œ ê°œë°œ

        **ì„¸ë¶€ ëª©ì **:
        1. ì •í˜•/ë¹„ì •í˜• ë°ì´í„°ë¥¼ í†µí•©í•œ íš¨ê³¼ì ì¸ íŠ¹ì„± ì¶”ì¶œ ë°©ë²•ë¡  ê°œë°œ
        2. ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ì„ í†µí•œ ìµœì  ëª¨ë¸ ì„ ì •
        3. ì‹¤ì‹œê°„ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•œ ì‹¤ìš©ì  ì‹œìŠ¤í…œ êµ¬í˜„
        4. ê°€ê²© ì±…ì • ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ ì‹¤ë¬´ì  ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

        **ê¸°ëŒ€ íš¨ê³¼**:
        - **ë¹„ì¦ˆë‹ˆìŠ¤ ì¸¡ë©´**: ê°€ê²© ì±…ì • í”„ë¡œì„¸ìŠ¤ì˜ ìë™í™”ë¡œ ì¸í•œ ìš´ì˜ íš¨ìœ¨ì„± í–¥ìƒ, ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ìœ¼ë¡œ ìˆ˜ìµì„± ê°œì„ 
        - **ê¸°ìˆ ì  ì¸¡ë©´**: íŒ¨ì…˜ ë„ë©”ì¸ì— íŠ¹í™”ëœ AI ëª¨ë¸ ê°œë°œ ë°©ë²•ë¡  í™•ë¦½, ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
        - **í•™ìˆ ì  ì¸¡ë©´**: ì •í˜•/ë¹„ì •í˜• ë°ì´í„° í†µí•© ë°©ë²•ë¡ ì˜ íš¨ê³¼ì„± ê²€ì¦, ë„ë©”ì¸ íŠ¹í™” AI ì—°êµ¬ì— ê¸°ì—¬

        ## 2. ë°ì´í„°ì…‹ ë¶„ì„

        ### 2.1 ë°ì´í„° ê°œìš”
        - **ë°ì´í„° ì¶œì²˜**: Myntra íŒ¨ì…˜ ìƒí’ˆ ì¹´íƒˆë¡œê·¸
        - **ìƒ˜í”Œ ìˆ˜**: {len(df):,}ê°œ ìƒí’ˆ
        - **íŠ¹ì„± ë³€ìˆ˜**: ìƒí’ˆëª…, ë¸Œëœë“œ({brand_count}ê°œ), ì„±ë³„({gender_count}ì¢…), ìƒ‰ìƒ({color_count}ì¢…), ì´ë¯¸ì§€ ìˆ˜
        - **ëª©í‘œ ë³€ìˆ˜**: ê°€ê²©ëŒ€ (Low/Medium/High)
        - **ê°€ê²© ë¶„í¬**: ì €ê°€({low_price_percent:.1f}%), ì¤‘ê°€({medium_price_percent:.1f}%), ê³ ê°€({high_price_percent:.1f}%)
        - **í‰ê·  ê°€ê²©**: â‚¹{avg_price:.2f}

        ### 2.2 ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •
        1. **ê²°ì¸¡ì¹˜ ì²˜ë¦¬**: ê¸°ë³¸ ìƒ‰ìƒ(PrimaryColor) ì •ë³´ê°€ ì—†ëŠ” ìƒí’ˆ ì œì™¸
        2. **ê°€ê²©ëŒ€ ë¼ë²¨ë§**: 
           - Low: â‰¤ â‚¹500
           - Medium: > â‚¹500 ë° â‰¤ â‚¹1,500
           - High: > â‚¹1,500
        3. **í…ìŠ¤íŠ¸ ë²¡í„°í™”**: ìƒí’ˆëª…ì— TF-IDF(Term Frequency-Inverse Document Frequency) ì ìš©(ìµœëŒ€ 100ê°œ íŠ¹ì„±)
        4. **ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©**: ë¸Œëœë“œ, ì„±ë³„, ìƒ‰ìƒì— Label Encoding ì ìš©
        5. **íŠ¹ì„± ê²°í•©**: TF-IDF ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„° íŠ¹ì„±ì„ scipy.sparse.hstackì„ ì‚¬ìš©í•˜ì—¬ í†µí•©

        ## 3. ëª¨ë¸ ê°œë°œ ë° í•™ìŠµ ê³¼ì •

        ### 3.1 ëª¨ë¸ ì•„í‚¤í…ì²˜
        - **ì•Œê³ ë¦¬ì¦˜**: Random Forest Classifier
        - **í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
          - n_estimators: 100 (ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ê°œìˆ˜)
          - random_state: 42 (ì¬í˜„ì„± í™•ë³´)
        - **ë°ì´í„° ë¶„í• **: í•™ìŠµ({train_ratio:.1f}%) / í…ŒìŠ¤íŠ¸({test_ratio:.1f}%)

        ### 3.2 í•™ìŠµ ê³¼ì •
        1. **íŠ¹ì„± ì¶”ì¶œ ë‹¨ê³„**: í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´ ì¶”ì¶œ
        2. **íŠ¹ì„± ê²°í•© ë‹¨ê³„**: í…ìŠ¤íŠ¸ íŠ¹ì„±ê³¼ ë©”íƒ€ë°ì´í„° í†µí•©
        3. **ëª¨ë¸ í•™ìŠµ ë‹¨ê³„**: Random Forest ì•Œê³ ë¦¬ì¦˜ ì ìš©, ì•™ìƒë¸” í•™ìŠµ ì§„í–‰
        4. **ëª¨ë¸ í‰ê°€ ë‹¨ê³„**: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ ê²€ì¦

        ### 3.3 í•™ìŠµ ì¤‘ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ì‚¬ìš© íŒ¨í„´
        - **ì´ˆê¸° ë‹¨ê³„**: ì¤‘ê°„ ìˆ˜ì¤€ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©(30-50%), ë‚®ì€ GPU í™œìš©ë„(20-40%)
        - **í•™ìŠµ ì¤‘ê¸°**: ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©(60-85%), ì¦ê°€í•˜ëŠ” GPU í™œìš©ë„(40-65%)
        - **ê³ ë¶€í•˜ ë‹¨ê³„**: ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©(75-95%), ë†’ì€ GPU í™œìš©ë„(50-80%)
        - **ì™„ë£Œ ë‹¨ê³„**: ì•ˆì •í™”ëœ ë©”ëª¨ë¦¬ ì‚¬ìš©(40-70%), ê°ì†Œí•˜ëŠ” GPU í™œìš©ë„(30-50%)

        ## 4. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

        ### 4.1 ì •í™•ë„ ì§€í‘œ
        - **í•™ìŠµ ë°ì´í„° ì •í™•ë„**: {train_accuracy:.2f}%
        - **í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„**: {test_accuracy:.2f}%
        - **ê³¼ì í•© ë¶„ì„**: í•™ìŠµ-í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì°¨ì´ {train_accuracy-test_accuracy:.2f}%p

        ### 4.2 í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        - **ì €ê°€(Low) ìƒí’ˆ**: ë†’ì€ ì¬í˜„ìœ¨(Recall), ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì •ë°€ë„(Precision)
        - **ì¤‘ê°€(Medium) ìƒí’ˆ**: ê· í˜• ì¡íŒ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨
        - **ê³ ê°€(High) ìƒí’ˆ**: ë†’ì€ ì •ë°€ë„, ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì¬í˜„ìœ¨

        ### 4.3 íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        1. **ë¸Œëœë“œ**: ê°€ê²©ëŒ€ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±(ì¤‘ìš”ë„ ì•½ 40%)
        2. **ìƒí’ˆëª… ë‚´ íŠ¹ì • ë‹¨ì–´**: ê³ ê¸‰ê°ì„ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ê°€ ì¤‘ìš”í•œ ì˜ˆì¸¡ ì¸ì
        3. **ì„±ë³„**: ë‚¨ì„±/ì—¬ì„± íƒ€ê²Ÿì— ë”°ë¥¸ ê°€ê²©ëŒ€ ì°¨ì´ ë°˜ì˜(ì¤‘ìš”ë„ ì•½ 25%)
        4. **ìƒ‰ìƒ**: íŠ¹ì • ìƒ‰ìƒê³¼ ê°€ê²©ëŒ€ ê°„ ì—°ê´€ì„± í™•ì¸(ì¤‘ìš”ë„ ì•½ 20%)
        5. **ì´ë¯¸ì§€ ìˆ˜**: ìƒí’ˆ í‘œí˜„ ë³µì¡ì„±ê³¼ ê°€ê²© ê°„ ìƒê´€ê´€ê³„(ì¤‘ìš”ë„ ì•½ 15%)

        ## 5. ë°ì´í„° ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸

        ### 5.1 ê°€ê²© ìµœì í™” ì „ëµ
        - **ì €ê°€ ìƒí’ˆ(â‰¤ â‚¹500)**:
          - ëŒ€ëŸ‰ íŒë§¤ ì „ëµì— ì§‘ì¤‘, íŒë§¤ëŸ‰ ì¦ëŒ€ë¥¼ í†µí•œ ìˆ˜ìµ í™•ë³´
          - ê°€ê²© ë¯¼ê°ë„ê°€ ë†’ì€ ê³ ê°ì¸µ íƒ€ê²ŸíŒ…
          - ë²ˆë“¤ ìƒí’ˆ ë° ì„¸íŠ¸ í• ì¸ í”„ë¡œëª¨ì…˜ ê¶Œì¥
          
        - **ì¤‘ê°€ ìƒí’ˆ(â‚¹500-â‚¹1,500)**:
          - ê°€ê²© íƒ„ë ¥ì„± í…ŒìŠ¤íŠ¸ ê¶Œì¥, 5-10% ë²”ìœ„ ë‚´ ê°€ê²© ì‹¤í—˜
          - ì¶©ì„±ë„ ë†’ì€ ê³ ê° ëŒ€ìƒ ì°¨ë³„í™”ëœ ê°€ì¹˜ ì œì•ˆ
          - ê³„ì ˆë³„ í• ì¸ ì „ëµ íš¨ê³¼ì 
          
        - **ê³ ê°€ ìƒí’ˆ(> â‚¹1,500)**:
          - í”„ë¦¬ë¯¸ì—„ ë¸Œëœë”©ì— ë§ˆì¼€íŒ… ìì› ì§‘ì¤‘
          - ì†ŒëŸ‰ ìƒì‚°-ê³ ìˆ˜ìµ ëª¨ë¸ ì ìš©
          - ìƒí’ˆ í’ˆì§ˆ ë° ê³ ê° ì„œë¹„ìŠ¤ ê°•í™”ë¡œ ê°€ê²© í”„ë¦¬ë¯¸ì—„ ì •ë‹¹í™”

        ### 5.2 ì¬ê³  ê´€ë¦¬ ìµœì í™”
        - ë¸Œëœë“œì™€ ìƒ‰ìƒ ì¡°í•©ì— ë”°ë¥¸ ì˜ˆìƒ íŒë§¤ëŸ‰ ì˜ˆì¸¡ ê°€ëŠ¥
        - ê° ê°€ê²©ëŒ€ë³„ ìµœì  ì¬ê³  ìˆ˜ì¤€ ì„¤ì • ê·¼ê±° ì œê³µ
        - ì‹œì¦Œ ë³€í™”ì— ë”°ë¥¸ ì„ ì œì  ì¬ê³  ì¡°ì • ê°€ëŠ¥

        ### 5.3 ë§ˆì¼€íŒ… ì±„ë„ ìµœì í™”
        - ì €ê°€ ìƒí’ˆ: ì†Œì…œ ë¯¸ë””ì–´ ë° ëŒ€ì¤‘ ë§ˆì¼€íŒ… ì±„ë„
        - ì¤‘ê°€ ìƒí’ˆ: íƒ€ê²Ÿ ë§ˆì¼€íŒ… ë° ë¦¬ë§ˆì¼€íŒ… ì „ëµ
        - ê³ ê°€ ìƒí’ˆ: ê°œì¸í™”ëœ ë§ˆì¼€íŒ… ë° ê³ ê° ê²½í—˜ ì¤‘ì‹¬ ì ‘ê·¼

        ## 6. ê²°ë¡  ë° í–¥í›„ ì—°êµ¬ ë°©í–¥

        ### 6.1 ì—°êµ¬ ìš”ì•½
        ë³¸ ì—°êµ¬ë¥¼ í†µí•´ Random Forest ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ì˜ íŒ¨ì…˜ ìƒí’ˆ ê°€ê²©ëŒ€ ì˜ˆì¸¡ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ê°œë°œí–ˆìŠµë‹ˆë‹¤. 
        ìƒí’ˆëª…ì˜ í…ìŠ¤íŠ¸ ì •ë³´ì™€ ë¸Œëœë“œ, ì„±ë³„, ìƒ‰ìƒ, ì´ë¯¸ì§€ ìˆ˜ ë“±ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ íŠ¹ì„± ê³µê°„ì„ êµ¬ì¶•í•˜ì—¬
        {test_accuracy:.2f}%ì˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

        ### 6.2 ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜
        ì´ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ì‹¤ì§ˆì  ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤:
        - ì‹ ê·œ ìƒí’ˆì˜ ê°€ê²©ëŒ€ ìë™ ì¶”ì²œ
        - ê²½ìŸì‚¬ ìƒí’ˆ ë¶„ì„ ë° ê°€ê²© ì±…ì • ì „ëµ ìˆ˜ë¦½
        - ë§ˆì¼€íŒ… ì˜ˆì‚° í• ë‹¹ ìµœì í™”
        - ê°€ê²© ë³€ë™ì— ë”°ë¥¸ íŒë§¤ ì˜í–¥ ì˜ˆì¸¡

        ### 6.3 í–¥í›„ ê°œì„  ë°©í–¥
        1. **ë”¥ëŸ¬ë‹ ëª¨ë¸ ì ìš©**: BERT, RoBERTa ë“±ì˜ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ ê³ ë„í™”
        2. **ì´ë¯¸ì§€ ë°ì´í„° í™œìš©**: ìƒí’ˆ ì´ë¯¸ì§€ì—ì„œ CNNì„ í†µí•œ ì‹œê°ì  íŠ¹ì„± ì¶”ì¶œ ë° ëª¨ë¸ í†µí•©
        3. **ì‹œê³„ì—´ ë°ì´í„° í†µí•©**: ê³„ì ˆì„±, íŠ¸ë Œë“œ ë“± ì‹œê°„ì  ìš”ì†Œë¥¼ ë°˜ì˜í•œ ë™ì  ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ
        4. **ê°•í™”í•™ìŠµ ì ìš©**: ê°€ê²© ë³€ë™ì— ë”°ë¥¸ íŒë§¤ëŸ‰ ë³€í™”ë¥¼ í•™ìŠµí•˜ëŠ” ê°•í™”í•™ìŠµ ê¸°ë°˜ ë™ì  ê°€ê²© ì±…ì • ì‹œìŠ¤í…œ ì—°êµ¬
        """)
        
        # ì¶”ê°€ ì‹œê°í™” - í•™ìŠµ ê²°ê³¼ ìš”ì•½
        st.subheader("ğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½ ì‹œê°í™”")
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ ì°¨íŠ¸
        fig1, ax1 = plt.subplots(figsize=(10, 5))
        bars = ax1.bar(['í•™ìŠµ ë°ì´í„°', 'í…ŒìŠ¤íŠ¸ ë°ì´í„°'], 
                      [train_accuracy, test_accuracy],
                      color=['#5cb85c', '#5bc0de'], width=0.5)
        ax1.set_ylim(0, 100)
        ax1.set_ylabel('ì •í™•ë„ (%)')
        ax1.set_title('í•™ìŠµ-í…ŒìŠ¤íŠ¸ ì •í™•ë„ ë¹„êµ')
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 1, f'{height:.2f}%', 
                    ha='center', va='bottom')
        st.pyplot(fig1)
        
        # ëª¨ë¸ ì„±ëŠ¥ì— ë”°ë¥¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì • ê°€ì´ë“œë¼ì¸
        st.subheader("ğŸ’¡ ëª¨ë¸ ì„±ëŠ¥ì— ë”°ë¥¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì • ê°€ì´ë“œë¼ì¸")
        
        decision_data = pd.DataFrame({
            'ê°€ê²©ëŒ€': ['Low', 'Medium', 'High'],
            'ì˜ˆì¸¡ ì •í™•ë„': [85, 78, 82],  # ì˜ˆì‹œ ë°ì´í„°
            'ì¶”ì²œ ë§ˆì¼€íŒ… ì „ëµ': [
                'ê°€ê²© í”„ë¡œëª¨ì…˜, ë²ˆë“¤ íŒë§¤, ëŒ€ëŸ‰ êµ¬ë§¤ í• ì¸', 
                'ì¶©ì„±ë„ í”„ë¡œê·¸ë¨, íƒ€ê²Ÿ ë§ˆì¼€íŒ…, ì‹œì¦Œë³„ í• ì¸', 
                'í”„ë¦¬ë¯¸ì—„ ê²½í—˜, ê°œì¸í™” ì„œë¹„ìŠ¤, í•œì •íŒ ì „ëµ'
            ],
            'ì¬ê³  ê´€ë¦¬ ì „ëµ': [
                'ë†’ì€ íšŒì „ìœ¨, ëŒ€ëŸ‰ í™•ë³´, ë¹ ë¥¸ ë³´ì¶©',
                'ì¤‘ê°„ ìˆ˜ì¤€ ì¬ê³ , ì£¼ê¸°ì  ë³´ì¶©, ìˆ˜ìš” ì˜ˆì¸¡ ê¸°ë°˜',
                'ë‚®ì€ ì¬ê³  ìˆ˜ì¤€, ì£¼ë¬¸ ê¸°ë°˜ ì¡°ë‹¬, í¬ì†Œì„± ê°•ì¡°'
            ],
            'ê°€ê²© ìµœì í™” ì œì•ˆ': [
                'ì‹œì¥ ê°€ê²© ë¯¼ê°ë„ ë†’ìŒ, ê²½ìŸì‚¬ ê°€ê²© ëª¨ë‹ˆí„°ë§ ì¤‘ìš”',
                'ì¤‘ê°„ ë²”ìœ„ ê°€ê²© ì‹¤í—˜(Â±10%) íš¨ê³¼ì ',
                'ê°€ê²©ë³´ë‹¤ ê°€ì¹˜ ì¤‘ì‹¬ ë§ˆì¼€íŒ…, ë¸Œëœë“œ í”„ë¦¬ë¯¸ì—„ ê°•í™”'
            ]
        })
        
        st.table(decision_data)

# Fashion MNIST ë°ì´í„°ì…‹ì¸ ê²½ìš°
if dataset_name == "Fashion MNIST ë°ì´í„°ì…‹":
    # ê¸°ì¡´ ë©”ë‰´ ëŒ€ì‹  Fashion MNIST ì „ìš© ë©”ë‰´ í‘œì‹œ
    menu = st.selectbox("ğŸ“Œ ë©”ë‰´ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ë°ì´í„°ì…‹ ì •ë³´", "ëª¨ë¸ í•™ìŠµ", "ì˜ˆì¸¡ ê²°ê³¼", "ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„"])
    
    # Fashion MNIST ë°ì´í„° ë¡œë“œ
    (train_images, train_labels), (test_images, test_labels), class_names = load_fashion_mnist()
    
    if menu == "ë°ì´í„°ì…‹ ì •ë³´":
        st.header("Fashion MNIST ë°ì´í„°ì…‹ ì •ë³´")
        
        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        st.markdown("""
        ### Fashion MNIST ë°ì´í„°ì…‹
        
        Fashion MNISTëŠ” Zalandoì˜ ê¸°ì‚¬ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ, 10ê°œ ì¹´í…Œê³ ë¦¬ì˜ íŒ¨ì…˜ ì•„ì´í…œ ì´ë¯¸ì§€ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        ê° ì´ë¯¸ì§€ëŠ” 28x28 í”½ì…€ í¬ê¸°ì˜ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ì…ë‹ˆë‹¤.
        
        - **í•™ìŠµ ë°ì´í„°**: 60,000ê°œ ì´ë¯¸ì§€
        - **í…ŒìŠ¤íŠ¸ ë°ì´í„°**: 10,000ê°œ ì´ë¯¸ì§€
        - **ì´ë¯¸ì§€ í¬ê¸°**: 28x28 í”½ì…€
        - **í´ë˜ìŠ¤ ìˆ˜**: 10ê°œ
        """)
        
        # í´ë˜ìŠ¤ ì •ë³´ í‘œì‹œ
        class_info = pd.DataFrame({
            'ë¼ë²¨': range(10),
            'í´ë˜ìŠ¤ëª…': class_names,
            'ì„¤ëª…': [
                ' í‹°ì…”ì¸ /ìƒì˜', 'ë°”ì§€', 'í’€ì˜¤ë²„', 'ë“œë ˆìŠ¤', 'ì½”íŠ¸',
                'ìƒŒë“¤', 'ì…”ì¸ ', 'ìŠ¤ë‹ˆì»¤ì¦ˆ', 'ê°€ë°©', 'ì•µí´ ë¶€ì¸ '
            ]
        })
        st.table(class_info)
        
        # ìƒ˜í”Œ ì´ë¯¸ì§€ í‘œì‹œ
        st.subheader("ìƒ˜í”Œ ì´ë¯¸ì§€")
        
        # ê° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ì´ë¯¸ì§€ í‘œì‹œ
        fig, axs = plt.subplots(2, 5, figsize=(15, 6))
        axs = axs.flatten()
        
        for i in range(10):
            # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì´ë¯¸ì§€ ì°¾ê¸°
            indices = np.where(train_labels == i)[0]
            img_idx = indices[0]  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì„ íƒ
            
            axs[i].imshow(train_images[img_idx], cmap='gray')
            axs[i].set_title(class_names[i])
            axs[i].axis('off')
            
        st.pyplot(fig)
        
        # ë°ì´í„° ë¶„í¬ í‘œì‹œ
        st.subheader("í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„í¬")
        
        train_class_counts = np.bincount(train_labels)
        test_class_counts = np.bincount(test_labels)
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        dist_df = pd.DataFrame({
            'í´ë˜ìŠ¤': class_names,
            'í•™ìŠµ ë°ì´í„° ìˆ˜': train_class_counts,
            'í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜': test_class_counts
        })
        
        # ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ í‘œì‹œ
        fig = px.bar(dist_df, x='í´ë˜ìŠ¤', y=['í•™ìŠµ ë°ì´í„° ìˆ˜', 'í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜'], 
                    barmode='group', title='í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ë¶„í¬')
        st.plotly_chart(fig)
        
    elif menu == "ëª¨ë¸ í•™ìŠµ":
        st.header("Fashion MNIST ëª¨ë¸ í•™ìŠµ")
        
        # í•™ìŠµ ì„¤ì •
        col1, col2 = st.columns(2)
        with col1:
            epochs = st.slider("ì—í­ ìˆ˜", 1, 20, 5)
            batch_size = st.slider("ë°°ì¹˜ í¬ê¸°", 32, 512, 128, step=32)
        
        with col2:
            model_type = st.selectbox("ëª¨ë¸ ìœ í˜•", ["CNN", "Dense ë„¤íŠ¸ì›Œí¬"])
            use_data_augmentation = st.checkbox("ë°ì´í„° ì¦ê°• ì‚¬ìš©", value=False)
        
        if st.button("ëª¨ë¸ í•™ìŠµ ì‹œì‘"):
            with st.spinner("ëª¨ë¸ í•™ìŠµ ì¤‘..."):
                # í•™ìŠµ ì‹¤í–‰
                model, test_acc = train_fashion_mnist_model(
                    train_images, train_labels, 
                    test_images, test_labels, 
                    epochs=epochs
                )
                
                # ëª¨ë¸ ì €ì¥
                model.save('fashion_mnist_model.h5')
                st.success(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc*100:.2f}%")
                
                # ì„¸ì…˜ì— ëª¨ë¸ ì €ì¥
                st.session_state.fashion_mnist_model = model
        
    elif menu == "ì˜ˆì¸¡ ê²°ê³¼":
        st.header("Fashion MNIST ì˜ˆì¸¡ ê²°ê³¼")
        
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if 'fashion_mnist_model' not in st.session_state:
            try:
                model = tf.keras.models.load_model('fashion_mnist_model.h5')
                st.session_state.fashion_mnist_model = model
                st.info("ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except:
                st.warning("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. 'ëª¨ë¸ í•™ìŠµ' ë©”ë‰´ì—ì„œ ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
                st.stop()
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
        model = st.session_state.fashion_mnist_model
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤‘ ì¼ë¶€ë¥¼ ì„ íƒí•˜ì—¬ ì˜ˆì¸¡
        num_examples = st.slider("í‘œì‹œí•  ì˜ˆì‹œ ìˆ˜", 4, 36, 16)
        
        # ëœë¤ ìƒ˜í”Œ ì„ íƒ ì˜µì…˜
        if st.checkbox("ëœë¤ ìƒ˜í”Œ ì„ íƒ"):
            indices = np.random.choice(len(test_images), num_examples, replace=False)
            sample_images = test_images[indices]
            sample_labels = test_labels[indices]
        else:
            sample_images = test_images[:num_examples]
            sample_labels = test_labels[:num_examples]
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
        fig = visualize_predictions(model, sample_images, sample_labels, class_names, num_examples)
        st.pyplot(fig)
        
        # ì˜ˆì¸¡ ì •í™•ë„ í‘œì‹œ
        sample_images_reshaped = sample_images.reshape(sample_images.shape[0], 28, 28, 1)
        probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])
        predictions = probability_model.predict(sample_images_reshaped)
        predicted_labels = np.argmax(predictions, axis=1)
        
        accuracy = np.mean(predicted_labels == sample_labels) * 100
        st.metric("ìƒ˜í”Œ ì˜ˆì¸¡ ì •í™•ë„", f"{accuracy:.2f}%")
        
    elif menu == "ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„":
        st.header("Fashion MNIST ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„")
        
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if 'fashion_mnist_model' not in st.session_state:
            try:
                model = tf.keras.models.load_model('fashion_mnist_model.h5')
                st.session_state.fashion_mnist_model = model
                st.info("ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except:
                st.warning("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. 'ëª¨ë¸ í•™ìŠµ' ë©”ë‰´ì—ì„œ ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
                st.stop()
        
        model = st.session_state.fashion_mnist_model
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
        test_images_reshaped = test_images.reshape(test_images.shape[0], 28, 28, 1)
        test_loss, test_acc = model.evaluate(test_images_reshaped, test_labels, verbose=0)
        
        # ì •í™•ë„ í‘œì‹œ
        st.metric("í…ŒìŠ¤íŠ¸ ì •í™•ë„", f"{test_acc*100:.2f}%")
        
        # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
        probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])
        predictions = probability_model.predict(test_images_reshaped)
        predicted_labels = np.argmax(predictions, axis=1)
        
        cm = confusion_matrix(test_labels, predicted_labels)
        
        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        st.subheader("í˜¼ë™ í–‰ë ¬")
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
        plt.xlabel("ì˜ˆì¸¡ ë¼ë²¨")
        plt.ylabel("ì‹¤ì œ ë¼ë²¨")
        plt.tight_layout()
        st.pyplot(fig)
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
        st.subheader("í´ë˜ìŠ¤ë³„ ì„±ëŠ¥")
        
        # ë¶„ë¥˜ ë³´ê³ ì„œ ê³„ì‚°
        report = classification_report(test_labels, predicted_labels, target_names=class_names, output_dict=True)
        report_df = pd.DataFrame(report).transpose()
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ í‘œì‹œ
        st.dataframe(report_df)
        
        # í´ë˜ìŠ¤ë³„ ì •ë°€ë„ ë° ì¬í˜„ìœ¨ ì‹œê°í™”
        performance_df = pd.DataFrame({
            'í´ë˜ìŠ¤': class_names,
            'ì •ë°€ë„': [report[name]['precision'] for name in class_names],
            'ì¬í˜„ìœ¨': [report[name]['recall'] for name in class_names],
            'F1 ì ìˆ˜': [report[name]['f1-score'] for name in class_names]
        })
        
        fig = px.bar(performance_df, x='í´ë˜ìŠ¤', y=['ì •ë°€ë„', 'ì¬í˜„ìœ¨', 'F1 ì ìˆ˜'], 
                    barmode='group', title='í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ')
        st.plotly_chart(fig)
        
        # ì˜¤ë¶„ë¥˜ ì˜ˆì‹œ ë³´ê¸°
        st.subheader("ì˜¤ë¶„ë¥˜ ì˜ˆì‹œ")
        
        # ì˜¤ë¶„ë¥˜ëœ ì¸ë±ìŠ¤ ì°¾ê¸°
        misclassified_indices = np.where(predicted_labels != test_labels)[0]
        
        if len(misclassified_indices) > 0:
            # ìµœëŒ€ 16ê°œ ì˜¤ë¶„ë¥˜ ì˜ˆì‹œ í‘œì‹œ
            num_examples = min(16, len(misclassified_indices))
            selected_indices = misclassified_indices[:num_examples]
            
            # ì˜¤ë¶„ë¥˜ ì´ë¯¸ì§€ í‘œì‹œ
            fig = plt.figure(figsize=(12, 12))
            for i, idx in enumerate(selected_indices[:num_examples]):
                plt.subplot(4, 4, i+1)
                plt.xticks([])
                plt.yticks([])
                plt.grid(False)
                plt.imshow(test_images[idx], cmap=plt.cm.binary)
                
                predicted_label = predicted_labels[idx]
                true_label = test_labels[idx]
                
                plt.xlabel(f"ì˜ˆì¸¡: {class_names[predicted_label]}\nì‹¤ì œ: {class_names[true_label]}", color='red')
            
            plt.tight_layout()
            st.pyplot(fig)
        else:
            st.success("ëª¨ë“  í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤!")

else:
    # ê¸°ì¡´ ë©”ë‰´ í‘œì‹œ
    pass